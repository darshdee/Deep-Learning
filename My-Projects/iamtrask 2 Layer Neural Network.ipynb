{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets learn how to implement a 1 layer neural network focusing on binary classification. Activation function used= Sigmoid\n",
    "Source: https://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2689864 ],\n",
       "       [ 0.36375058],\n",
       "       [ 0.23762817],\n",
       "       [ 0.3262757 ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if (deriv==True):\n",
    "        return X*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "# Question: why return x *1(1-x) ?\n",
    "\n",
    "\n",
    "# Now we create our input and output datasets\n",
    "# input\n",
    "X = np.array([[0,0,1],\n",
    "             [0,1,1],\n",
    "             [1,0,1],\n",
    "             [1,1,1]])\n",
    "\n",
    "# output. Remember that you have to transpose it. Because it would then have each value correspond as the\n",
    "# ...target feature for each input example (row)\n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "# Seed random numbers to make a calculation - easier to see changes\n",
    "np.random.seed(1)\n",
    "\n",
    "# lets Initialize the weights/coeffs/W values. Remember its better to randomize than to set them = 0\n",
    "# ...we multiply by 2 and then subtract 1 to minimize the randomized values (?)\n",
    "W1 = 2 * np.random.random((3,1)) -1\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(10000):\n",
    "    \n",
    "    # lets begin forward propogation here----\n",
    "\n",
    "    # we classify the first layer of the network which is technically the input data (x)\n",
    "    X    \n",
    "    \n",
    "    # Then we classify the real first layer of the network (hidden layer)\n",
    "    #...lets calculate Z1. Note here that we do not take into account B for now \n",
    "    Z1 = np.dot(X,W1)\n",
    "    \n",
    "    #Lets calculate the error rate here\n",
    "    Z1_error = Y-Z1\n",
    "    \n",
    "    \"\"\"\n",
    "    Now theres 2 things to do here. \n",
    "    - Simply apply the sigmoid function here\n",
    "    - use the error weighted derivative method\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying the sigmoid function formula here\n",
    "    A1_simple = 1/(1+np.exp(-Z1))\n",
    "\n",
    "\n",
    "A1_simple    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Refer to the code from Source and experiment with this\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print (\"Output After Training:\")\n",
    "print (l1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
