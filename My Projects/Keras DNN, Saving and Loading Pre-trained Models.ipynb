{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Keras, Saving Models for Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprocessing Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49425287],\n",
       "       [0.75862069],\n",
       "       [0.10344828],\n",
       "       ...,\n",
       "       [0.89655172],\n",
       "       [0.43678161],\n",
       "       [0.63218391]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing data with Keras\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "\"\"\"\n",
    "CREATING TRAINING DATA\n",
    "\n",
    "- an experimental drug was tested on individuals from ages 13 to 65\n",
    "- the trial had 2100 participants. Half were under 65 yeards old, half were over 65 years old\n",
    "- 95% of patients 65 or older experienced side effects\n",
    "- 95% of patients under 65 experienced no side effects\n",
    "\n",
    "PREDICTING: new patient will experience side effects or not? - Binary classification problem\n",
    "\"\"\"\n",
    "\n",
    "for i in range(50):\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)    \n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)\n",
    "\n",
    "    \n",
    "size_training_data = len(train_samples)\n",
    "print (\"Size of training data:\", size_training_data)\n",
    "\n",
    "# converting the lists into numpy arrays before passing through the keras model\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Now we proprocess data! - always preprocess if your data has different units. Obviously dont normalize the labels\n",
    "# Also you have to reshape 1D arrays\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform((train_samples).reshape(-1,1))\n",
    "\n",
    "# take a look at the training samples\n",
    "scaled_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test data: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09195402],\n",
       "       [0.68965517],\n",
       "       [0.17241379],\n",
       "       [0.87356322],\n",
       "       [0.11494253],\n",
       "       [0.82758621],\n",
       "       [0.1954023 ],\n",
       "       [1.        ],\n",
       "       [0.49425287],\n",
       "       [0.81609195],\n",
       "       [0.03448276],\n",
       "       [0.89655172],\n",
       "       [0.49425287],\n",
       "       [0.86206897],\n",
       "       [0.26436782],\n",
       "       [0.63218391],\n",
       "       [0.37931034],\n",
       "       [0.68965517],\n",
       "       [0.12643678],\n",
       "       [0.97701149],\n",
       "       [0.1954023 ],\n",
       "       [0.98850575],\n",
       "       [0.01149425],\n",
       "       [1.        ],\n",
       "       [0.55172414],\n",
       "       [0.72413793],\n",
       "       [0.34482759],\n",
       "       [0.97701149],\n",
       "       [0.55172414],\n",
       "       [0.71264368],\n",
       "       [0.04597701],\n",
       "       [0.95402299],\n",
       "       [0.24137931],\n",
       "       [0.8045977 ],\n",
       "       [0.01149425],\n",
       "       [0.67816092],\n",
       "       [0.04597701],\n",
       "       [0.63218391],\n",
       "       [0.32183908],\n",
       "       [0.97701149],\n",
       "       [0.34482759],\n",
       "       [0.75862069],\n",
       "       [0.25287356],\n",
       "       [0.93103448],\n",
       "       [0.36781609],\n",
       "       [0.95402299],\n",
       "       [0.01149425],\n",
       "       [0.68965517],\n",
       "       [0.4137931 ],\n",
       "       [0.73563218],\n",
       "       [0.34482759],\n",
       "       [0.68965517],\n",
       "       [0.56321839],\n",
       "       [0.68965517],\n",
       "       [0.4137931 ],\n",
       "       [0.6091954 ],\n",
       "       [0.57471264],\n",
       "       [0.79310345],\n",
       "       [0.2183908 ],\n",
       "       [0.6091954 ],\n",
       "       [0.28735632],\n",
       "       [0.91954023],\n",
       "       [0.28735632],\n",
       "       [0.73563218],\n",
       "       [0.4137931 ],\n",
       "       [0.68965517],\n",
       "       [0.20689655],\n",
       "       [0.62068966],\n",
       "       [0.10344828],\n",
       "       [0.67816092],\n",
       "       [0.35632184],\n",
       "       [0.62068966],\n",
       "       [0.1954023 ],\n",
       "       [0.71264368],\n",
       "       [0.43678161],\n",
       "       [0.7816092 ],\n",
       "       [0.42528736],\n",
       "       [0.79310345],\n",
       "       [0.3908046 ],\n",
       "       [0.96551724],\n",
       "       [0.45977011],\n",
       "       [0.63218391],\n",
       "       [0.45977011],\n",
       "       [0.65517241],\n",
       "       [0.42528736],\n",
       "       [0.90804598],\n",
       "       [0.        ],\n",
       "       [0.7816092 ],\n",
       "       [0.        ],\n",
       "       [0.71264368],\n",
       "       [0.48275862],\n",
       "       [0.6091954 ],\n",
       "       [0.09195402],\n",
       "       [0.79310345],\n",
       "       [0.49425287],\n",
       "       [0.63218391],\n",
       "       [0.34482759],\n",
       "       [0.66666667],\n",
       "       [0.54022989],\n",
       "       [0.64367816],\n",
       "       [0.16091954],\n",
       "       [0.63218391],\n",
       "       [0.35632184],\n",
       "       [0.93103448],\n",
       "       [0.20689655],\n",
       "       [0.68965517],\n",
       "       [0.        ],\n",
       "       [0.94252874],\n",
       "       [0.52873563],\n",
       "       [0.95402299],\n",
       "       [0.09195402],\n",
       "       [0.83908046],\n",
       "       [0.        ],\n",
       "       [0.82758621],\n",
       "       [0.24137931],\n",
       "       [0.85057471],\n",
       "       [0.        ],\n",
       "       [0.93103448],\n",
       "       [0.02298851],\n",
       "       [0.6091954 ],\n",
       "       [0.28735632],\n",
       "       [0.66666667],\n",
       "       [0.12643678],\n",
       "       [0.71264368],\n",
       "       [0.06896552],\n",
       "       [0.68965517],\n",
       "       [0.54022989],\n",
       "       [0.74712644],\n",
       "       [0.36781609],\n",
       "       [0.66666667],\n",
       "       [0.36781609],\n",
       "       [0.91954023],\n",
       "       [0.31034483],\n",
       "       [0.65517241],\n",
       "       [0.57471264],\n",
       "       [0.90804598],\n",
       "       [0.57471264],\n",
       "       [0.70114943],\n",
       "       [0.33333333],\n",
       "       [0.85057471],\n",
       "       [0.45977011],\n",
       "       [0.6091954 ],\n",
       "       [0.13793103],\n",
       "       [0.98850575],\n",
       "       [0.45977011],\n",
       "       [0.82758621],\n",
       "       [0.4137931 ],\n",
       "       [0.68965517],\n",
       "       [0.08045977],\n",
       "       [0.90804598],\n",
       "       [0.12643678],\n",
       "       [0.72413793],\n",
       "       [0.26436782],\n",
       "       [0.85057471],\n",
       "       [0.11494253],\n",
       "       [0.68965517],\n",
       "       [0.24137931],\n",
       "       [0.91954023],\n",
       "       [0.57471264],\n",
       "       [0.93103448],\n",
       "       [0.22988506],\n",
       "       [0.63218391],\n",
       "       [0.11494253],\n",
       "       [0.77011494],\n",
       "       [0.42528736],\n",
       "       [0.86206897],\n",
       "       [0.16091954],\n",
       "       [0.70114943],\n",
       "       [0.01149425],\n",
       "       [0.75862069],\n",
       "       [0.04597701],\n",
       "       [0.70114943],\n",
       "       [0.55172414],\n",
       "       [0.98850575],\n",
       "       [0.05747126],\n",
       "       [0.6091954 ],\n",
       "       [0.40229885],\n",
       "       [0.75862069],\n",
       "       [0.32183908],\n",
       "       [0.73563218],\n",
       "       [0.51724138],\n",
       "       [0.59770115],\n",
       "       [0.54022989],\n",
       "       [0.59770115],\n",
       "       [0.25287356],\n",
       "       [0.70114943],\n",
       "       [0.04597701],\n",
       "       [0.68965517],\n",
       "       [0.04597701],\n",
       "       [0.95402299],\n",
       "       [0.04597701],\n",
       "       [0.66666667],\n",
       "       [0.16091954],\n",
       "       [0.93103448],\n",
       "       [0.06896552],\n",
       "       [0.86206897],\n",
       "       [0.52873563],\n",
       "       [0.63218391],\n",
       "       [0.43678161],\n",
       "       [0.79310345],\n",
       "       [0.43678161],\n",
       "       [0.59770115],\n",
       "       [0.4137931 ],\n",
       "       [0.64367816],\n",
       "       [0.02298851],\n",
       "       [0.85057471],\n",
       "       [0.52873563],\n",
       "       [0.79310345],\n",
       "       [0.10344828],\n",
       "       [0.6091954 ],\n",
       "       [0.50574713],\n",
       "       [0.73563218],\n",
       "       [0.09195402],\n",
       "       [0.68965517],\n",
       "       [0.48275862],\n",
       "       [0.86206897],\n",
       "       [0.18390805],\n",
       "       [0.6091954 ],\n",
       "       [0.57471264],\n",
       "       [0.65517241],\n",
       "       [0.37931034],\n",
       "       [0.91954023],\n",
       "       [0.01149425],\n",
       "       [0.93103448],\n",
       "       [0.5862069 ],\n",
       "       [0.74712644],\n",
       "       [0.57471264],\n",
       "       [0.87356322],\n",
       "       [0.17241379],\n",
       "       [0.77011494],\n",
       "       [0.50574713],\n",
       "       [0.72413793],\n",
       "       [0.        ],\n",
       "       [0.66666667],\n",
       "       [0.14942529],\n",
       "       [0.64367816],\n",
       "       [0.40229885],\n",
       "       [0.68965517],\n",
       "       [0.51724138],\n",
       "       [0.82758621],\n",
       "       [0.31034483],\n",
       "       [0.88505747],\n",
       "       [0.51724138],\n",
       "       [0.70114943],\n",
       "       [0.42528736],\n",
       "       [0.95402299],\n",
       "       [0.10344828],\n",
       "       [0.8045977 ],\n",
       "       [0.54022989],\n",
       "       [0.72413793],\n",
       "       [0.45977011],\n",
       "       [0.90804598],\n",
       "       [0.31034483],\n",
       "       [0.98850575],\n",
       "       [0.24137931],\n",
       "       [0.83908046],\n",
       "       [0.20689655],\n",
       "       [0.82758621],\n",
       "       [0.12643678],\n",
       "       [0.91954023],\n",
       "       [0.36781609],\n",
       "       [0.98850575],\n",
       "       [0.20689655],\n",
       "       [0.75862069],\n",
       "       [0.4137931 ],\n",
       "       [0.86206897],\n",
       "       [0.43678161],\n",
       "       [0.87356322],\n",
       "       [0.04597701],\n",
       "       [0.63218391],\n",
       "       [0.06896552],\n",
       "       [0.67816092],\n",
       "       [0.04597701],\n",
       "       [0.88505747],\n",
       "       [0.17241379],\n",
       "       [0.74712644],\n",
       "       [0.24137931],\n",
       "       [0.91954023],\n",
       "       [0.        ],\n",
       "       [0.8045977 ],\n",
       "       [0.51724138],\n",
       "       [0.65517241],\n",
       "       [0.10344828],\n",
       "       [0.85057471],\n",
       "       [0.45977011],\n",
       "       [0.74712644],\n",
       "       [0.2183908 ],\n",
       "       [0.87356322],\n",
       "       [0.18390805],\n",
       "       [0.7816092 ],\n",
       "       [0.47126437],\n",
       "       [0.71264368],\n",
       "       [0.32183908],\n",
       "       [0.62068966],\n",
       "       [0.20689655],\n",
       "       [0.64367816],\n",
       "       [0.35632184],\n",
       "       [0.94252874],\n",
       "       [0.11494253],\n",
       "       [0.83908046],\n",
       "       [0.08045977],\n",
       "       [0.63218391],\n",
       "       [0.42528736],\n",
       "       [0.82758621],\n",
       "       [0.40229885],\n",
       "       [0.65517241],\n",
       "       [0.24137931],\n",
       "       [0.6091954 ],\n",
       "       [0.18390805],\n",
       "       [0.73563218],\n",
       "       [0.09195402],\n",
       "       [0.90804598],\n",
       "       [0.20689655],\n",
       "       [0.90804598],\n",
       "       [0.52873563],\n",
       "       [0.64367816],\n",
       "       [0.16091954],\n",
       "       [0.68965517],\n",
       "       [0.28735632],\n",
       "       [0.82758621],\n",
       "       [0.28735632],\n",
       "       [0.72413793],\n",
       "       [0.20689655],\n",
       "       [1.        ],\n",
       "       [0.52873563],\n",
       "       [0.87356322],\n",
       "       [0.17241379],\n",
       "       [0.71264368],\n",
       "       [0.50574713],\n",
       "       [0.6091954 ],\n",
       "       [0.16091954],\n",
       "       [0.63218391],\n",
       "       [0.45977011],\n",
       "       [0.8045977 ],\n",
       "       [0.03448276],\n",
       "       [1.        ],\n",
       "       [0.32183908],\n",
       "       [0.64367816],\n",
       "       [0.50574713],\n",
       "       [0.71264368],\n",
       "       [0.40229885],\n",
       "       [0.8045977 ],\n",
       "       [0.57471264],\n",
       "       [0.94252874],\n",
       "       [0.40229885],\n",
       "       [0.64367816],\n",
       "       [0.47126437],\n",
       "       [0.90804598],\n",
       "       [0.45977011],\n",
       "       [0.8045977 ],\n",
       "       [0.31034483],\n",
       "       [0.75862069],\n",
       "       [0.17241379],\n",
       "       [0.93103448],\n",
       "       [0.35632184],\n",
       "       [0.79310345],\n",
       "       [0.26436782],\n",
       "       [0.72413793],\n",
       "       [0.47126437],\n",
       "       [0.73563218],\n",
       "       [0.2183908 ],\n",
       "       [1.        ],\n",
       "       [0.33333333],\n",
       "       [0.67816092],\n",
       "       [0.3908046 ],\n",
       "       [0.7816092 ],\n",
       "       [0.43678161],\n",
       "       [0.88505747],\n",
       "       [0.2183908 ],\n",
       "       [0.83908046],\n",
       "       [0.14942529],\n",
       "       [0.74712644],\n",
       "       [0.35632184],\n",
       "       [0.83908046],\n",
       "       [0.55172414],\n",
       "       [0.6091954 ],\n",
       "       [0.4137931 ],\n",
       "       [0.81609195],\n",
       "       [0.09195402],\n",
       "       [0.6091954 ],\n",
       "       [0.16091954],\n",
       "       [0.66666667],\n",
       "       [0.05747126],\n",
       "       [0.98850575],\n",
       "       [0.05747126],\n",
       "       [0.7816092 ],\n",
       "       [0.36781609],\n",
       "       [0.66666667],\n",
       "       [0.52873563],\n",
       "       [0.86206897],\n",
       "       [0.55172414],\n",
       "       [0.64367816],\n",
       "       [0.01149425],\n",
       "       [0.71264368],\n",
       "       [0.36781609],\n",
       "       [1.        ],\n",
       "       [0.34482759],\n",
       "       [0.91954023],\n",
       "       [0.29885057],\n",
       "       [0.82758621],\n",
       "       [0.49425287],\n",
       "       [0.82758621],\n",
       "       [0.18390805],\n",
       "       [0.87356322],\n",
       "       [0.05747126],\n",
       "       [0.88505747],\n",
       "       [0.13793103],\n",
       "       [0.64367816],\n",
       "       [0.3908046 ],\n",
       "       [0.70114943],\n",
       "       [0.22988506],\n",
       "       [0.88505747],\n",
       "       [0.36781609],\n",
       "       [0.70114943],\n",
       "       [0.45977011],\n",
       "       [0.81609195],\n",
       "       [0.29885057],\n",
       "       [0.96551724],\n",
       "       [0.50574713],\n",
       "       [0.73563218]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(10):\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)    \n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "\n",
    "    \n",
    "size_test_data = len(test_samples)\n",
    "print (\"Size of test data:\", size_test_data)\n",
    "\n",
    "# converting the lists into numpy arrays before passing through the keras model\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Now we proprocess data! - always preprocess if your data has different units. Obviously dont normalize the labels\n",
    "# Also you have to reshape 1D arrays\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_test_samples = scaler.fit_transform((test_samples).reshape(-1,1))\n",
    "\n",
    "# take a look at the test samples\n",
    "scaled_test_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Our Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to train and predict using our First Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1680 samples, validate on 420 samples\n",
      "Epoch 1/40\n",
      " - 0s - loss: 0.6352 - acc: 0.6095 - val_loss: 0.5615 - val_acc: 0.7738\n",
      "Epoch 2/40\n",
      " - 0s - loss: 0.5306 - acc: 0.7899 - val_loss: 0.4244 - val_acc: 0.9048\n",
      "Epoch 3/40\n",
      " - 0s - loss: 0.4186 - acc: 0.8756 - val_loss: 0.2920 - val_acc: 0.9476\n",
      "Epoch 4/40\n",
      " - 0s - loss: 0.3339 - acc: 0.9071 - val_loss: 0.2018 - val_acc: 0.9476\n",
      "Epoch 5/40\n",
      " - 0s - loss: 0.2994 - acc: 0.9196 - val_loss: 0.1651 - val_acc: 0.9690\n",
      "Epoch 6/40\n",
      " - 0s - loss: 0.2863 - acc: 0.9190 - val_loss: 0.1498 - val_acc: 0.9881\n",
      "Epoch 7/40\n",
      " - 0s - loss: 0.2826 - acc: 0.9321 - val_loss: 0.1372 - val_acc: 0.9810\n",
      "Epoch 8/40\n",
      " - 0s - loss: 0.2789 - acc: 0.9238 - val_loss: 0.1342 - val_acc: 0.9881\n",
      "Epoch 9/40\n",
      " - 0s - loss: 0.2758 - acc: 0.9327 - val_loss: 0.1274 - val_acc: 0.9810\n",
      "Epoch 10/40\n",
      " - 0s - loss: 0.2739 - acc: 0.9274 - val_loss: 0.1253 - val_acc: 0.9810\n",
      "Epoch 11/40\n",
      " - 0s - loss: 0.2725 - acc: 0.9292 - val_loss: 0.1235 - val_acc: 0.9810\n",
      "Epoch 12/40\n",
      " - 0s - loss: 0.2710 - acc: 0.9274 - val_loss: 0.1222 - val_acc: 0.9881\n",
      "Epoch 13/40\n",
      " - 0s - loss: 0.2700 - acc: 0.9333 - val_loss: 0.1235 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      " - 0s - loss: 0.2678 - acc: 0.9310 - val_loss: 0.1220 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      " - 0s - loss: 0.2687 - acc: 0.9310 - val_loss: 0.1165 - val_acc: 0.9881\n",
      "Epoch 16/40\n",
      " - 0s - loss: 0.2665 - acc: 0.9333 - val_loss: 0.1151 - val_acc: 0.9881\n",
      "Epoch 17/40\n",
      " - 0s - loss: 0.2661 - acc: 0.9333 - val_loss: 0.1148 - val_acc: 0.9810\n",
      "Epoch 18/40\n",
      " - 0s - loss: 0.2648 - acc: 0.9298 - val_loss: 0.1163 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      " - 0s - loss: 0.2650 - acc: 0.9339 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      " - 0s - loss: 0.2645 - acc: 0.9345 - val_loss: 0.1125 - val_acc: 0.9881\n",
      "Epoch 21/40\n",
      " - 0s - loss: 0.2637 - acc: 0.9304 - val_loss: 0.1160 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      " - 0s - loss: 0.2639 - acc: 0.9321 - val_loss: 0.1108 - val_acc: 0.9881\n",
      "Epoch 23/40\n",
      " - 0s - loss: 0.2635 - acc: 0.9339 - val_loss: 0.1088 - val_acc: 0.9881\n",
      "Epoch 24/40\n",
      " - 0s - loss: 0.2639 - acc: 0.9304 - val_loss: 0.1070 - val_acc: 0.9881\n",
      "Epoch 25/40\n",
      " - 0s - loss: 0.2642 - acc: 0.9298 - val_loss: 0.1101 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      " - 0s - loss: 0.2629 - acc: 0.9315 - val_loss: 0.1118 - val_acc: 0.9905\n",
      "Epoch 27/40\n",
      " - 0s - loss: 0.2626 - acc: 0.9339 - val_loss: 0.1064 - val_acc: 0.9881\n",
      "Epoch 28/40\n",
      " - 0s - loss: 0.2619 - acc: 0.9369 - val_loss: 0.1074 - val_acc: 0.9881\n",
      "Epoch 29/40\n",
      " - 0s - loss: 0.2629 - acc: 0.9345 - val_loss: 0.1073 - val_acc: 0.9881\n",
      "Epoch 30/40\n",
      " - 0s - loss: 0.2640 - acc: 0.9298 - val_loss: 0.1041 - val_acc: 0.9881\n",
      "Epoch 31/40\n",
      " - 0s - loss: 0.2618 - acc: 0.9321 - val_loss: 0.1085 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      " - 0s - loss: 0.2619 - acc: 0.9333 - val_loss: 0.1056 - val_acc: 0.9810\n",
      "Epoch 33/40\n",
      " - 0s - loss: 0.2616 - acc: 0.9357 - val_loss: 0.1089 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      " - 0s - loss: 0.2618 - acc: 0.9351 - val_loss: 0.1064 - val_acc: 0.9881\n",
      "Epoch 35/40\n",
      " - 0s - loss: 0.2617 - acc: 0.9315 - val_loss: 0.1047 - val_acc: 0.9810\n",
      "Epoch 36/40\n",
      " - 0s - loss: 0.2611 - acc: 0.9310 - val_loss: 0.1067 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      " - 0s - loss: 0.2611 - acc: 0.9345 - val_loss: 0.1056 - val_acc: 0.9881\n",
      "Epoch 38/40\n",
      " - 0s - loss: 0.2619 - acc: 0.9357 - val_loss: 0.1047 - val_acc: 0.9905\n",
      "Epoch 39/40\n",
      " - 0s - loss: 0.2602 - acc: 0.9327 - val_loss: 0.1021 - val_acc: 0.9881\n",
      "Epoch 40/40\n",
      " - 0s - loss: 0.2607 - acc: 0.9375 - val_loss: 0.1046 - val_acc: 0.9881\n"
     ]
    }
   ],
   "source": [
    "# Creating our DNN\n",
    "model = Sequential([\n",
    "    Dense(16,input_shape =(1,), activation = 'relu'),\n",
    "    Dense(32,activation = 'relu'),\n",
    "    Dense(2,activation='softmax')\n",
    "    ])\n",
    "\n",
    "\"\"\"\n",
    "You can also add layers through models like this or the method above\n",
    "\n",
    "model.add(l4)\n",
    "model.add(l5)\n",
    "etc\n",
    "\"\"\"\n",
    "\n",
    "# checking our number of training parameters and visualizing our model\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model\n",
    "\"\"\"\n",
    "# Compiling\n",
    "- optimizer (learning rate)\n",
    "- loss function\n",
    "- metrics: array of metrics you want to use to judge the model. Look up documentation\n",
    "\"\"\"\n",
    "model.compile(Adam(lr=0.001), loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# The training step which includes the validaton set. validation_split automatically splits up x% of samples\n",
    "# Shuffle is default true anyways,in each epoch the order is randomized but doesn't work 'with validation data\n",
    "model.fit(scaled_train_samples,train_labels, batch_size = 20,validation_split=0.20, epochs=40, shuffle=True, verbose=2)    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ANOTHER WAY TO CREATE A VALIDATION SET\n",
    "\n",
    "# creating a validation set\n",
    "\n",
    "# One way to do it\n",
    "valid_set = [(0.23,0),(0.454,1)]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NOW WE PREDICT\n",
    "- Note that model.predict in this case return 2 columns - probabiltiy of both classes\n",
    "- Also note that we use model.predict_classes to actually predict the class label. \n",
    "It automatically picks the max probability as the class\n",
    "\n",
    "Why do we need batch size for keras prediction? - for computation purposes. Predict batch at a time as opposed to all together\n",
    "\n",
    "\"\"\"\n",
    "predictions = model.predict(scaled_test_samples, batch_size=20,verbose=0)\n",
    "\n",
    "predictions_classes = model.predict_classes(scaled_test_samples, batch_size=10,verbose=0)\n",
    "# for i in predictions_classes:\n",
    "#     print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the accuracy of our simple binary classification model using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[199  11]\n",
      " [ 10 200]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We copy paste the code off of scikit learns wesbite to plot the confusion matrix\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_labels, predictions_classes)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, Interpolation = 'nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "        print ('normalized confusion matrix')\n",
    "        \n",
    "    else:\n",
    "        print ('Confusion matrix, without normalization')        \n",
    "    print (cm)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j,i,cm[1,j], horizontalalignment = 'center',\n",
    "                color = 'white' if cm[i,j] > thresh else 'black')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "cm_plot_labels = ['no side effects', 'had side effects']\n",
    "plot_confusion_matrix(cm,cm_plot_labels,title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading a Model\n",
    "##### It is important to learn how to save a model and load it as a pre-trained model elsewhere in other application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL.SAVE\n",
    "- saves the architecture\n",
    "- saves the weights\n",
    "- training configuration (hyperparameter settings)\n",
    "- state of the optimizer! Which means you can begin optimizing from the weights where paused!\n",
    "\"\"\"\n",
    "model.save('C:/Users/Darshil/gitly/Deep-Learning/My Projects/medical_trial_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.Adam at 0x175ac0f25f8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL.LOAD\n",
    "- loading the model we saved\n",
    "\"\"\"\n",
    "from keras.models import load_model\n",
    "new_model = load_model('C:/Users/Darshil/gitly/Deep-Learning/My Projects/medical_trial_model.h5')\n",
    "\n",
    "# now lets look at the model history, weights and which optimizer it used\n",
    "new_model.summary()\n",
    "new_model.get_weights()\n",
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving only the model (architecture) ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Sequential\", \"config\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_16\", \"trainable\": true, \"batch_input_shape\": [null, 1], \"dtype\": \"float32\", \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_17\", \"trainable\": true, \"units\": 32, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_18\", \"trainable\": true, \"units\": 2, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}], \"keras_version\": \"2.2.0\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do this by saving as a JSON file\n",
    "json_string = model.to_json()\n",
    "\n",
    "# You can also save as a yaml file\n",
    "# yaml_string = model.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Lets reconstruct the model from JSON\n",
    "from keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "model_architecture.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the weights ONLY\n",
    "The whole here is to be able to load these weights into a new model of the SAME architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('C:/Users/Darshil/gitly/Deep-Learning/My Projects/saved_medical_trial_model.h5')\n",
    "\n",
    "# making a new model- have to explicitly doing so\n",
    "model2 = Sequential([\n",
    "    Dense(16,input_shape =(1,), activation = 'relu'),\n",
    "    Dense(32,activation = 'relu'),\n",
    "    Dense(2,activation='softmax')\n",
    "    ])\n",
    "\n",
    "# passing the pretrained weights into the new model\n",
    "model2.load_weights('C:/Users/Darshil/gitly/Deep-Learning/My Projects/saved_medical_trial_model.h5')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "FYI \n",
    "- In order to do partial transfer learning \n",
    "\n",
    "\"\n",
    "On a layer you can call layer.get_weights() to get the weights from that layer. \n",
    "You can also call layer.set_weights(weights) to set the weights of a layer. \n",
    "So you could use get_weights() from one layer, \n",
    "and set weights in another layer with set_weights()... \n",
    "using the result from get_weights().\n",
    "\"\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
