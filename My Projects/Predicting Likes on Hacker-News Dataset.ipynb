{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Regression Model on Hacker News Dataset\n",
    "- Predicting upvotes by using the bag of words model with post title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#random\n",
    "import random\n",
    "\n",
    "#Natural language processing: Next we import packages to convert the json data from json file and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in Our Dataset\n",
    "The dataset has over 200K post titles with the respective number of likes (or number of points) and the number of comments. Since its not possible to train a dataset this huge on the local computer, we're going to have to subset the data\n",
    "\n",
    "[dataset](https://www.kaggle.com/hacker-news/hacker-news-posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>num_points</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Appropriate Uses for SQLite</td>\n",
       "      <td>125</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>UnGoogled Chromium: Chromium with enhanced pri...</td>\n",
       "      <td>251</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Designing and producing 2FA tokens to sell on ...</td>\n",
       "      <td>138</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Finance is Not the Economy</td>\n",
       "      <td>237</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>The decline of Stack Overflow (2015)</td>\n",
       "      <td>291</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Bidirectional Replication is coming to Postgre...</td>\n",
       "      <td>200</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Bay Area wages soaring but still cant keep up ...</td>\n",
       "      <td>103</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Park.io  automating tasks to make $125k per month</td>\n",
       "      <td>342</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Game Genie declassified: That summer I played ...</td>\n",
       "      <td>156</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>John Carmack .plan Archive (2014)</td>\n",
       "      <td>122</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  num_points  \\\n",
       "67                         Appropriate Uses for SQLite         125   \n",
       "88   UnGoogled Chromium: Chromium with enhanced pri...         251   \n",
       "127  Designing and producing 2FA tokens to sell on ...         138   \n",
       "158                         Finance is Not the Economy         237   \n",
       "239               The decline of Stack Overflow (2015)         291   \n",
       "240  Bidirectional Replication is coming to Postgre...         200   \n",
       "275  Bay Area wages soaring but still cant keep up ...         103   \n",
       "282  Park.io  automating tasks to make $125k per month         342   \n",
       "288  Game Genie declassified: That summer I played ...         156   \n",
       "295                  John Carmack .plan Archive (2014)         122   \n",
       "\n",
       "     num_comments  \n",
       "67             56  \n",
       "88            120  \n",
       "127            52  \n",
       "158           115  \n",
       "239           255  \n",
       "240            38  \n",
       "275           166  \n",
       "282           146  \n",
       "288            40  \n",
       "295            38  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hack_data = pd.read_csv('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/project_specific/E2_predict_likes_hackernews/hn_posts.csv')\n",
    "hack_data[hack_data['num_points']>100].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we extract our features we need to subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  num_points  \\\n",
      "145665  That Time an SR-71 Made an Emergency Landing i...         276   \n",
      "88088   Sci-Hub Is Blowing Up the Academic Publishing ...         291   \n",
      "19198   How a Japanese cucumber farmer is using deep l...         423   \n",
      "9262                              Why are Adults so busy?         508   \n",
      "68881   The British are Googling what the E.U. is, hou...         413   \n",
      "\n",
      "        num_comments  \n",
      "145665            75  \n",
      "88088            115  \n",
      "19198            127  \n",
      "9262             471  \n",
      "68881            568   \n",
      " \n",
      " (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "#for now we'll take the first 1000 and then implement later. Idea: concat 700 above 200 and 300 below\n",
    "\n",
    "#breakdown - make change here only\n",
    "thresh = 200\n",
    "above_200 = 1000\n",
    "total = 2000\n",
    "\n",
    "#creating our training data and concating\n",
    "#pick 700 above above 200\n",
    "df1 = hack_data[(hack_data['num_points']> thresh)]\n",
    "df1 = df1.loc[random.sample(list(df1.index),above_200)]\n",
    "\n",
    "#pick 300 below 200\n",
    "df2 = hack_data[(hack_data['num_points']<= thresh)]\n",
    "df2 = df2.loc[random.sample(list(df2.index),total - above_200)]\n",
    "\n",
    "#concating both dfs into our training dataset\n",
    "df = pd.concat([df1, df2])\n",
    "print (df.head() ,'\\n \\n' ,df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am work'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"st = PorterStemmer()\n",
    "b = 'i am working'\n",
    "\n",
    "txt = \" \".join([st.stem(w) for w in b.split(' ')])\n",
    "txt\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we'll apply the bag of words model on our title column bit before we do that we need to clean our data: \n",
    "- remove all stop words\n",
    "- remove words with length < 4\n",
    "- remove punctuations\n",
    "\"\"\"\n",
    "\n",
    "# We will now begin cleaning up our 2 columns namely \"headline\" and \"short_description\"\n",
    "punctuations_list = [\",\", \":\", \";\", \".\", \"'\", '\"', \"â€™\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\", \"/\"]\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#Here we have created a function that will clean up any given pandases series\n",
    "def preprocess_textcol(name):\n",
    "    st = PorterStemmer()\n",
    "    df['new_{}'.format(name)] = df[name].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in sw\n",
    "                                                                             and word.lower() not in sw \n",
    "                                                                             and word.upper() not in sw\n",
    "                                                                             and word.title() not in sw\n",
    "                                                                         and word not in punctuations_list and (len(word)> 5) \n",
    "                                                                  and word != '']))  #numbers doesnt work    \n",
    "    \n",
    "    #applying stemming\n",
    "    #df['new_{}'.format(name)] = df['new_{}'.format(name)].apply(lambda x: ' '.join([st.stem(word) for word in x.split(' ')]))\n",
    "        \n",
    "    return df['new_{}'.format(name)] \n",
    "\n",
    "#creating 2 new columns out of the original columns \"headlines\" and \"short_description\"\n",
    "df['new_title'] = preprocess_textcol('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145665                        Emergency Landing Norway\n",
       "88088     Sci-Hub Blowing Academic Publishing Industry\n",
       "19198     Japanese cucumber farmer learning TensorFlow\n",
       "9262                                            Adults\n",
       "68881                          British Googling voting\n",
       "101686                               fucking webmaster\n",
       "149675                                 Gneural Network\n",
       "256992                  abusing Unicode create tragedy\n",
       "218569                    Bullshit Startup Experiences\n",
       "11270                      Dynamic Programming: (1984)\n",
       "Name: new_title, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view our new data\n",
    "df['new_title'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we need to create our bag of words model and create a count vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 302)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=4)\n",
    "data_corpus = list(df['new_title'].values.astype('U'))\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "print(X.toarray())\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "training = pd.DataFrame(X.toarray(), columns =vectorizer.get_feature_names() )\n",
    "training.shape\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving this to local so we deploy it in flask for the count vec function\n",
    "try:\n",
    "    deploy_df = df.drop(['num_points','num_comments','title'], axis=1)\n",
    "except:\n",
    "    print ('already dropped')\n",
    "\n",
    "deploy_df.to_csv('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/project_specific/E2_predict_likes_hackernews/deploy_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last but not the least we need to train it using scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#call model, fit and train\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(training,df['num_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[517.79947579 197.01124095]\n"
     ]
    }
   ],
   "source": [
    "# Now we predict!\n",
    "test_val = vectorizer.transform(['google google google google algorithm algorithm', 'adults busy']) \n",
    "#print ('New value: ', test_val.toarray()[0])\n",
    "print (model.predict(test_val.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 302)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_vec(text):    \n",
    "    t = vectorizer.transform([text])\n",
    "    return t.toarray()\n",
    "\n",
    "#model.predict(convert_vec('google'))\n",
    "convert_vec('text').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is horrible we will need to train it on a much larger dataset\n",
    "#### Next we need to save this model to deploy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predict_likes_hackernews']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model,'predict_likes_hackernews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Example code for count vectorization (from other project)\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", 'darshil also likes to watch movies']\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "test_df = pd.DataFrame(X.toarray(), columns =vectorizer.get_feature_names() )\n",
    "test_df\n",
    "\n",
    "#new values\n",
    "new_val = vectorizer.transform(['hello likes to']) \n",
    "print ('New value: ', new_val.toarray()[0])\n",
    "\n",
    "project1_env\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
