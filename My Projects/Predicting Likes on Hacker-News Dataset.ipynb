{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Regression Model on Hacker News Dataset\n",
    "- Predicting upvotes by using the bag of words model with post title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#random\n",
    "import random\n",
    "\n",
    "#Natural language processing: Next we import packages to convert the json data from json file and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in Our Dataset\n",
    "The dataset has over 200K post titles with the respective number of likes (or number of points) and the number of comments. Since its not possible to train a dataset this huge on the local computer, we're going to have to subset the data\n",
    "\n",
    "[dataset](https://www.kaggle.com/hacker-news/hacker-news-posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>num_points</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Appropriate Uses for SQLite</td>\n",
       "      <td>125</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>UnGoogled Chromium: Chromium with enhanced pri...</td>\n",
       "      <td>251</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Designing and producing 2FA tokens to sell on ...</td>\n",
       "      <td>138</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Finance is Not the Economy</td>\n",
       "      <td>237</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>The decline of Stack Overflow (2015)</td>\n",
       "      <td>291</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Bidirectional Replication is coming to Postgre...</td>\n",
       "      <td>200</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Bay Area wages soaring but still cant keep up ...</td>\n",
       "      <td>103</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Park.io  automating tasks to make $125k per month</td>\n",
       "      <td>342</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Game Genie declassified: That summer I played ...</td>\n",
       "      <td>156</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>John Carmack .plan Archive (2014)</td>\n",
       "      <td>122</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  num_points  \\\n",
       "67                         Appropriate Uses for SQLite         125   \n",
       "88   UnGoogled Chromium: Chromium with enhanced pri...         251   \n",
       "127  Designing and producing 2FA tokens to sell on ...         138   \n",
       "158                         Finance is Not the Economy         237   \n",
       "239               The decline of Stack Overflow (2015)         291   \n",
       "240  Bidirectional Replication is coming to Postgre...         200   \n",
       "275  Bay Area wages soaring but still cant keep up ...         103   \n",
       "282  Park.io  automating tasks to make $125k per month         342   \n",
       "288  Game Genie declassified: That summer I played ...         156   \n",
       "295                  John Carmack .plan Archive (2014)         122   \n",
       "\n",
       "     num_comments  \n",
       "67             56  \n",
       "88            120  \n",
       "127            52  \n",
       "158           115  \n",
       "239           255  \n",
       "240            38  \n",
       "275           166  \n",
       "282           146  \n",
       "288            40  \n",
       "295            38  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hack_data = pd.read_csv('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/project_specific/E2_predict_likes_hackernews/hn_posts.csv')\n",
    "hack_data[hack_data['num_points']>100].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we extract our features we need to subset the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  num_points  \\\n",
      "1746    Akamai takes Brian Krebs site off its servers ...         649   \n",
      "259597             Goodbye CoffeeScript, Hello TypeScript         327   \n",
      "276498                                        Amazon Flex         593   \n",
      "140409                   Privacy  Forget Your Credit Card         661   \n",
      "22567   What Slack might learn from its Open Source al...         219   \n",
      "\n",
      "        num_comments  \n",
      "1746             435  \n",
      "259597           256  \n",
      "276498           460  \n",
      "140409           359  \n",
      "22567             94   \n",
      " \n",
      " (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "#for now we'll take the first 1000 and then implement later. Idea: concat 700 above 200 and 300 below\n",
    "\n",
    "#breakdown - make change here only\n",
    "thresh = 200\n",
    "above_thresh = 1000\n",
    "total = 2000\n",
    "\n",
    "#creating our training data and concating\n",
    "#pick 700 above above 200\n",
    "df1 = hack_data[(hack_data['num_points']> thresh)]\n",
    "df1 = df1.loc[random.sample(list(df1.index),above_thresh)]\n",
    "\n",
    "#pick 300 below 200\n",
    "df2 = hack_data[(hack_data['num_points']<= thresh)]\n",
    "df2 = df2.loc[random.sample(list(df2.index),total - above_200)]\n",
    "\n",
    "#concating both dfs into our training dataset\n",
    "df = pd.concat([df1, df2])\n",
    "print (df.head() ,'\\n \\n' ,df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am work'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#quick example of how stemming works\n",
    "\"\"\"st = PorterStemmer()\n",
    "b = 'i am working'\n",
    "\n",
    "txt = \" \".join([st.stem(w) for w in b.split(' ')])\n",
    "txt\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we'll apply the bag of words model on our title column bit before we do that we need to clean our data: \n",
    "- remove all stop words\n",
    "- remove words with length < 4\n",
    "- remove punctuations\n",
    "\"\"\"\n",
    "\n",
    "# We will now begin cleaning up our 2 columns namely \"headline\" and \"short_description\"\n",
    "punctuations_list = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\", \"/\"]\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#Here we have created a function that will clean up any given pandases series\n",
    "def preprocess_textcol(name):\n",
    "    st = PorterStemmer()\n",
    "    df['new_{}'.format(name)] = df[name].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in sw\n",
    "                                                                             and word.lower() not in sw \n",
    "                                                                             and word.upper() not in sw\n",
    "                                                                             and word.title() not in sw\n",
    "                                                                         and word not in punctuations_list and (len(word)> 5) \n",
    "                                                                  and word != '']))  #numbers doesnt work    \n",
    "    \n",
    "    \n",
    "    # removing digits\n",
    "    df['new_{}'.format(name)] = df['new_{}'.format(name)].str.replace('\\d+', '')\n",
    "    \n",
    "    #applying stemming\n",
    "    df['new_{}'.format(name)] = df['new_{}'.format(name)].apply(lambda x: ' '.join([st.stem(word) for word in x.split(' ')]))\n",
    "    \n",
    "    #removing null values\n",
    "    df['new_{}'.format(name)] = df['new_{}'.format(name)].dropna()\n",
    "    \n",
    "    return df['new_{}'.format(name)] \n",
    "\n",
    "#creating 2 new columns out of the original columns \"headlines\" and \"short_description\"\n",
    "df['new_title'] = preprocess_textcol('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746                    akamai server record cyberattack\n",
       "259597                   goodby coffeescript, typescript\n",
       "276498                                            amazon\n",
       "140409                             privaci forget credit\n",
       "22567                                       sourc altern\n",
       "153564                   cocoapod download github server\n",
       "50650                             dollar disrupt everyth\n",
       "112291                                    technic recipi\n",
       "58992     bayesian analysi racial polic shoot unit state\n",
       "37647         googl outpac facebook get connect internet\n",
       "Name: new_title, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view our new data\n",
    "df['new_title'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we need to create our bag of words model and create a count vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>acquir</th>\n",
       "      <th>actual</th>\n",
       "      <th>address</th>\n",
       "      <th>advic</th>\n",
       "      <th>algebra</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>altern</th>\n",
       "      <th>amazon</th>\n",
       "      <th>...</th>\n",
       "      <th>whatsapp</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>window</th>\n",
       "      <th>without</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>youtub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 338 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accept  access  acquir  actual  address  advic  algebra  algorithm  altern  \\\n",
       "0       0       0       0       0        0      0        0          0       0   \n",
       "1       0       0       0       0        0      0        0          0       0   \n",
       "2       0       0       0       0        0      0        0          0       0   \n",
       "3       0       0       0       0        0      0        0          0       0   \n",
       "4       0       0       0       0        0      0        0          0       1   \n",
       "\n",
       "   amazon   ...    whatsapp  wikipedia  window  without  work  worker  world  \\\n",
       "0       0   ...           0          0       0        0     0       0      0   \n",
       "1       0   ...           0          0       0        0     0       0      0   \n",
       "2       1   ...           0          0       0        0     0       0      0   \n",
       "3       0   ...           0          0       0        0     0       0      0   \n",
       "4       0   ...           0          0       0        0     0       0      0   \n",
       "\n",
       "   write  year  youtub  \n",
       "0      0     0       0  \n",
       "1      0     0       0  \n",
       "2      0     0       0  \n",
       "3      0     0       0  \n",
       "4      0     0       0  \n",
       "\n",
       "[5 rows x 338 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=4)\n",
    "data_corpus = list(df['new_title'].values.astype('U'))\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "#print(X.toarray())\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "training = pd.DataFrame(X.toarray(), columns =vectorizer.get_feature_names() )\n",
    "training.head()\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIDE NOTE: Saving this to local so we deploy it in flask for the count vec function\n",
    "try:\n",
    "    deploy_df = df.drop(['num_points','num_comments','title'], axis=1)\n",
    "except:\n",
    "    print ('already dropped')\n",
    "\n",
    "deploy_df.to_csv('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/project_specific/E2_predict_likes_hackernews/deploy_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last but not the least we need to train it using scikit learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#call model, fit and train\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(training,df['num_points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207.59466337 112.365509  ]\n"
     ]
    }
   ],
   "source": [
    "# Now we predict!\n",
    "st = PorterStemmer()\n",
    "test_sentences = [st.stem(word) for word in ['President Trump met with North korea leader', 'adults busy']]\n",
    "\n",
    "test_val = vectorizer.transform(test_sentences) \n",
    "#print ('New value: ', test_val.toarray()[0])\n",
    "print (model.predict(test_val.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_vec(text):    \n",
    "    t = vectorizer.transform( [st.stem(word) for word in [text]])\n",
    "    return t.toarray()\n",
    "\n",
    "#model.predict(convert_vec('google'))\n",
    "convert_vec('address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_vec('address')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is horrible we will need to train it on a much larger dataset\n",
    "#### Next we need to save this model to deploy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/saved_models/predict_likes_hackernews']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model,'C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/saved_models/predict_likes_hackernews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([244.09686361])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "hacknews_model = joblib.load('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask-app/saved_models/predict_likes_hackernews')\n",
    "hacknews_model.predict([convert_vec('google')][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[convert_vec('google')][0][0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Example code for count vectorization (from other project)\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", 'darshil also likes to watch movies']\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "test_df = pd.DataFrame(X.toarray(), columns =vectorizer.get_feature_names() )\n",
    "test_df\n",
    "\n",
    "#new values\n",
    "new_val = vectorizer.transform(['hello likes to']) \n",
    "print ('New value: ', new_val.toarray()[0])\n",
    "\n",
    "project1_env\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
