{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines Notes \n",
    "- Darshil Desai\n",
    "\n",
    "Only put in stuff that is new to you, since you are already familiar with model fitting and training- do not overlap existing knowledge! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.feature_extraction import CountV \n",
    "from sklearn.feature_selection import f_classif, chi2, SelectKBest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "credit = pd.read_csv('credit.csv')\n",
    "flows = pd.read_csv('lanl_flows.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Label Encoding\n",
    "- Similar to Pandas \"to_categorical\" method, this feature in scikit-learn allows you to numerically represent your categorical variables!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   checking_status  duration  credit_history  purpose  credit_amount  \\\n",
       "0                1         6               1        4           1169   \n",
       "\n",
       "   savings_status  employment  installment_commitment  personal_status  \\\n",
       "0               4           3                       4                3   \n",
       "\n",
       "   other_parties  ...  property_magnitude  age  other_payment_plans  housing  \\\n",
       "0              2  ...                   2   67                    1        1   \n",
       "\n",
       "   existing_credits  job  num_dependents  own_telephone  foreign_worker  class  \n",
       "0                 2    3               1              1               1   good  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_columns = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status',\n",
    "                      'other_parties', 'property_magnitude', 'other_payment_plans','housing', 'job', 'own_telephone','foreign_worker']\n",
    "\n",
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])\n",
    "    \n",
    "credit.head(1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Param Grid\n",
    "\n",
    "- This is an important concept that allows you to iterate over the HYPER-PARAMETERS of the model you choose and returns the one\n",
    "with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y = credit['class']\n",
    "    X = credit.drop('class', axis=1)    \n",
    "except:\n",
    "    print ('class col dropped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Set a range for n_estimators from 10 to 40 in steps of 10\n",
    "param_grid = {'n_estimators': list(range(10, 50,10))}  # the hyper parameter here = n_estimators\n",
    "\n",
    "# Not you define a model and simply plug in the hyper param values to iterate over\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "\n",
    "grid.fit(X, y)\n",
    "grid.best_params_  # the one that yields the best accuracy. \n",
    "\n",
    "\n",
    "# Question: does grid search work for contonous predictor variables? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical Encodings\n",
    "This represents one-hot encoded categorical features! - Know this so will skip over explaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(\n",
    "  credit['credit_history'])   # here we first transform the cats into numerical representation. Although not neccessary\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], 1)\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])],1)\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames\n",
    "X_hot.shape[1] > X_num.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. f_classif - Statistical method for feature selection\n",
    "Basically this helps us pick features that are continuous! Similar to how we pick categorical variables using chi-square tests\n",
    "\n",
    "Note: \n",
    "\"IF the features are quantitative, compute the ANOVA F-value between each feature and the target vector....The F-value scores examine **IF**, when we group the numerical feature by the target vector, the means for each group are significantly different.\"  - [chris_albon](https://chrisalbon.com/machine_learning/feature_selection/anova_f-value_for_feature_selection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x- np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['credit_amount_diff'] = abs_diff(credit.credit_amount)\n",
    "\n",
    "# Score old and new versions of this feature with f_classif()\n",
    "scores = f_classif( credit[['credit_amount', 'credit_amount_diff']], credit['class'])[0]\n",
    "\n",
    "# Inspect the scores and drop the lowest-scoring feature\n",
    "credit_new = credit.drop(['credit_amount'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------ -------------------------- ------------------------ --------------------- ---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advanced Data Wrangling\n",
    "\n",
    "- Take a look at some smart ways to wrangle data so you can draw inspiration from it in your feature work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>duration</th>\n",
       "      <th>source_computer</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_computer</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>packet_count</th>\n",
       "      <th>byte_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24128</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N17023</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N2414</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N19148</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24156</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N8001</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  duration source_computer source_port destination_computer  \\\n",
       "0  471692         0           C5808      N24128               C26871   \n",
       "1  471692         0           C5808       N2414               C26871   \n",
       "2  471692         0           C5808      N24156               C26871   \n",
       "\n",
       "  destination_port  protocol  packet_count  byte_count  \n",
       "0           N17023         6             1          60  \n",
       "1           N19148         6             1          60  \n",
       "2            N8001         6             1          60  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002F397917F60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some function to apply on a dataframe\n",
    "def featurize(df):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and returns a dictionary with relevant information\n",
    "    \"\"\"\n",
    "    return {\n",
    "    'unique_ports': len(set(df['destination_port'])),\n",
    "    'average_packet': np.mean(df['packet_count']),\n",
    "    'average_duration': np.mean(df['duration'])\n",
    "    }\n",
    "\n",
    "#Group by source computer, and apply the feature extractor\n",
    "out = flows.groupby('source_computer')  # This is an iterator object where each object is a dataframe grouped by the \n",
    "                                        #.....\"sourced computer\" variable\n",
    "    \n",
    "out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Using apply to literaly apply the \"featurize\" function on top on each dataframe\n",
    "    - Note the total number of rows, that means the iterator object had that many dataframes as each object!\n",
    "\"\"\"\n",
    "\n",
    "out = flows.groupby('source_computer').apply(featurize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_computer\n",
       "C10026    {'unique_ports': 2, 'average_packet': 21.0, 'a...\n",
       "C10047    {'unique_ports': 5, 'average_packet': 21.07692...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the iterator to a dataframe by calling list on it\n",
    "X = pd.DataFrame(list(out), index= out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_duration</th>\n",
       "      <th>average_packet</th>\n",
       "      <th>unique_ports</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_computer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C10026</th>\n",
       "      <td>39.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 average_duration  average_packet  unique_ports\n",
       "source_computer                                                \n",
       "C10                           5.0           222.0             4\n",
       "C10026                       39.0            21.0             2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = ['C9945', 'C9723', 'C977', 'C10']\n",
    "\n",
    "# Check which sources in X.index are bad to create labels\n",
    "y = [x in bads for x in X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966414564404514\n"
     ]
    }
   ],
   "source": [
    "# Report the average accuracy of Adaboost over 3-fold CV\n",
    "print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advanced Feature Engineering\n",
    "\n",
    "- Section in focus covers noteworthy ways in which to data wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966414564404514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SECTION IN FOCUS------------------------------\n",
    "# Create a feature counting unique protocols per source\n",
    "protocols = flows.groupby('source_computer').apply(\n",
    "  lambda df: len(set(df.protocol)) )\n",
    "\n",
    "\"\"\"\n",
    "1. First the groupby simply creates an iterator object where each object is a df\n",
    "2. next you use apply and lambda to calculate the number of unique values in the \"protocol\" column in EACH object/df !\n",
    "\n",
    "So now each row has source_computer or ID and the unique protocols which is then added onto the main feature set\n",
    "\"\"\"\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "# Convert this feature into a dataframe, naming the column\n",
    "protocols_DF = pd.DataFrame(\n",
    "  protocols, index=protocols.index, columns=['protocol'])\n",
    "\n",
    "# Now concatenate this feature with the previous dataset, X\n",
    "X_more = pd.concat([X, protocols_DF], axis=1)  # could also use merge...\n",
    "\n",
    "# Refit the classifier and report its accuracy\n",
    "print(np.mean(cross_val_score(\n",
    "  AdaBoostClassifier(), X_more, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting implementation of the uniqly package! Figure out how to iterate over all columns in one passing\n",
    "\n",
    "protocols1 = flows.groupby('source_computer').apply(\n",
    "  lambda df: list(set(df.protocol)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sample Weights \n",
    "\n",
    "The idea here is that we can weight or give importance to some labels more than others! \n",
    "\n",
    "- For example: : _\"One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as \"noisy\". Thankfully you know how to do that, using weighted learning. \"_\n",
    "\n",
    "    So here essentially in our entire training dataset the first 100 labels are \"iffy\" but the rest of the labels from 101 to whatever are fine! So we can still use this model by applying weights to the first 100 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765\n",
      "0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Lets say that the first 100 labels were labeled using a heuristic process or some unreliable process\n",
    "y_train_noisy = y_train  # assuming right\n",
    "\n",
    "# Fit a classifier to the training data\n",
    "clf = RandomForestClassifier().fit(X_train, y_train_noisy)\n",
    "\n",
    "# Report its accuracy on the test data\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Section in focus----------------------------------\n",
    "    # Assign half the weight to the first 100 noisy examples\n",
    "weights = [0.5]*100 + [1.0]*(len(y_train)-100)\n",
    "\n",
    "\"\"\"\n",
    "Here we set the weights of the first 100 to 0.5 and the rest after 100 to 1 which means normal!\n",
    "- The weights are assigned arbitarily and intuitively \n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "# Refit using weights and report accuracy. Has it improved?\n",
    "clf_weights = RandomForestClassifier().fit(X_train, y_train_noisy, sample_weight=weights)\n",
    "print(accuracy_score(y_test, clf_weights.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Classification Metrics: Confusion Matrix \n",
    "\n",
    "- Quick runthrough of how to go about returning classification metrics using SKlearn.\n",
    "\n",
    "**Context**  _a customer who defaulted on their loan, and a \"negative\" means a customer who continued to pay without problems. The bank manager informed you that the bank makes 10K profit on average from each \"good risk\" customer, but loses 150K from each \"bad risk\" customer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote the concept of cost: In our case, since a bad customer identified as good is more expensive than a good customer identified \\nas good, the cost value above assigns a monetary value ($-thousands) to the overall classficiation metrics\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a random forest classifier to the training data\n",
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "\n",
    "# Label the test data\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Section in focus------------------------------------\n",
    "    # Get confusion matrix only\n",
    "confusion_matrix(y_test, preds)\n",
    "\n",
    "    # Get false positives/negatives from the confusion matrix\n",
    "tp, fp, fn, tn = confusion_matrix(y_test, preds).ravel()  # using ravel flattens the confusion matrix \n",
    "\n",
    "# Now compute the cost using the manager's advice\n",
    "cost = fp*10 + fn*150\n",
    "\n",
    "\"\"\"\n",
    "Note the concept of cost: In our case, since a bad customer identified as good is more expensive than a good customer identified \n",
    "as good, the cost value above assigns a monetary value ($-thousands) to the overall classficiation metrics\n",
    "\"\"\"\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimizing Classification Thresholds\n",
    "\n",
    "The concept of optimizing classification thresholds refers to the idea that in order to reduce false positives or fine tune your confusion matrix properties- you can manually specify or iterate over different threshold values and use that to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "scores = clf.predict_proba(X_test)\n",
    "y_test = y_test == 'bad'\n",
    "\n",
    "# Section in focus ------------------------------------\n",
    "# Create a range of equally spaced threshold values\n",
    "t_range = [0,0.25,0.5,0.75,1]\n",
    "\n",
    "# Store the predicted labels for each value of the threshold\n",
    "preds = [[s[1] > thr for s in scores] for thr in t_range]\n",
    "\n",
    "# Compute the accuracy for each threshold\n",
    "accuracies = [accuracy_score(y_test, p) for p in preds]\n",
    "\n",
    "# Compute the F1 score for each threshold\n",
    "f1_scores = [f1_score(y_test, p) for p in preds]\n",
    "\n",
    "\"\"\"\n",
    "1. First we create a list of threshold values to iterate over\n",
    "2. In a nested list comprehension we use the prediction probabiities we got from predict_proba and \n",
    "simply assign 0 or 1 if the predicted probability of the class being 1 > threshold\n",
    "3. Then we calculate the accuracy score for each and figure out which threshold yielded the best accu and f1scores!\n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "# Report the optimal threshold for accuracy, and for F1\n",
    "print(t_range[np.argmax(accuracies)], t_range[np.argmax(f1_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------- -------------- -------------- -------------- --------------\n",
    "\n",
    "# Chapter 3 \n",
    "- Model Life Cycle Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Machine Learning Pipelines\n",
    "\n",
    "The big idea here is to be able to iterate over different versions of models, within which you iterate over >1 types of hyperparamters and the get the best one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10,20],\n",
    "    'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialize the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "1. First we set up a pipeline instance wherein we want to iterate over 2 things: number of continous features & \n",
    "...the n_estimator hyperparameter of the RandomForestClassifier model. \n",
    "    \n",
    "2. Then we create the parameters that will be iterated OVER. \n",
    "    Example: f\n",
    "    - feature_selection__k will have 10 and 20 iterated over as the K/number of features to select using SelectKBest(f_classif). \n",
    "    - clf__n_estimators: same here the n_estimator parameter iterates over values 2,5  \n",
    "\n",
    "    Keep in mind:\n",
    "    => While naming parameters always to \"pipelineName__K\"\n",
    "    => Note that all possible combinations are covered! So dont include lots of pipeline features as it will drastically \n",
    "    push the computational expense!\n",
    "\"\"\"\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train,y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
