{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines Notes \n",
    "- Darshil Desai\n",
    "\n",
    "Only put in stuff that is new to you, since you are already familiar with model fitting and training- do not overlap existing knowledge! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection import f_classif, chi2, SelectKBest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, make_scorer,roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof, DistanceMetric as dm\n",
    "from scipy.spatial.distance import squareform, pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "credit = pd.read_csv('credit.csv')\n",
    "flows = pd.read_csv('lanl_flows.csv')\n",
    "proteins = pd.read_csv('proteins_exercises.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Label Encoding\n",
    "- Similar to Pandas \"to_categorical\" method, this feature in scikit-learn allows you to numerically represent your categorical variables!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1169</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   checking_status  duration  credit_history  purpose  credit_amount  \\\n",
       "0                1         6               1        4           1169   \n",
       "\n",
       "   savings_status  employment  installment_commitment  personal_status  \\\n",
       "0               4           3                       4                3   \n",
       "\n",
       "   other_parties  ...  property_magnitude  age  other_payment_plans  housing  \\\n",
       "0              2  ...                   2   67                    1        1   \n",
       "\n",
       "   existing_credits  job  num_dependents  own_telephone  foreign_worker  class  \n",
       "0                 2    3               1              1               1   good  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numeric_columns = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status',\n",
    "                      'other_parties', 'property_magnitude', 'other_payment_plans','housing', 'job', 'own_telephone','foreign_worker']\n",
    "\n",
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])\n",
    "    \n",
    "credit.head(1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Param Grid\n",
    "\n",
    "- This is an important concept that allows you to iterate over the HYPER-PARAMETERS of the model you choose and returns the one\n",
    "with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    y = credit['class']\n",
    "    X = credit.drop('class', axis=1)    \n",
    "except:\n",
    "    print ('class col dropped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Set a range for n_estimators from 10 to 40 in steps of 10\n",
    "param_grid = {'n_estimators': list(range(10, 50,10))}  # the hyper parameter here = n_estimators\n",
    "\n",
    "# Not you define a model and simply plug in the hyper param values to iterate over\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "\n",
    "grid.fit(X, y)\n",
    "grid.best_params_  # the one that yields the best accuracy. \n",
    "\n",
    "\n",
    "# Question: does grid search work for contonous predictor variables? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical Encodings\n",
    "This represents one-hot encoded categorical features! - Know this so will skip over explaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(\n",
    "  credit['credit_history'])   # here we first transform the cats into numerical representation. Although not neccessary\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], 1)\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])],1)\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames\n",
    "X_hot.shape[1] > X_num.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. f_classif - Statistical method for feature selection\n",
    "Basically this helps us pick features that are continuous! Similar to how we pick categorical variables using chi-square tests\n",
    "\n",
    "Note: \n",
    "\"IF the features are quantitative, compute the ANOVA F-value between each feature and the target vector....The F-value scores examine **IF**, when we group the numerical feature by the target vector, the means for each group are significantly different.\"  - [chris_albon](https://chrisalbon.com/machine_learning/feature_selection/anova_f-value_for_feature_selection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x- np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['credit_amount_diff'] = abs_diff(credit.credit_amount)\n",
    "\n",
    "# Score old and new versions of this feature with f_classif()\n",
    "scores = f_classif( credit[['credit_amount', 'credit_amount_diff']], credit['class'])[0]\n",
    "\n",
    "# Inspect the scores and drop the lowest-scoring feature\n",
    "credit_new = credit.drop(['credit_amount'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------ -------------------------- ------------------------ --------------------- ---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advanced Data Wrangling\n",
    "\n",
    "- Take a look at some smart ways to wrangle data so you can draw inspiration from it in your feature work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>duration</th>\n",
       "      <th>source_computer</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_computer</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>packet_count</th>\n",
       "      <th>byte_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24128</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N17023</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N2414</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N19148</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24156</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N8001</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  duration source_computer source_port destination_computer  \\\n",
       "0  471692         0           C5808      N24128               C26871   \n",
       "1  471692         0           C5808       N2414               C26871   \n",
       "2  471692         0           C5808      N24156               C26871   \n",
       "\n",
       "  destination_port  protocol  packet_count  byte_count  \n",
       "0           N17023         6             1          60  \n",
       "1           N19148         6             1          60  \n",
       "2            N8001         6             1          60  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000019476A0DC88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some function to apply on a dataframe\n",
    "def featurize(df):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and returns a dictionary with relevant information\n",
    "    \"\"\"\n",
    "    return {\n",
    "    'unique_ports': len(set(df['destination_port'])),\n",
    "    'average_packet': np.mean(df['packet_count']),\n",
    "    'average_duration': np.mean(df['duration'])\n",
    "    }\n",
    "\n",
    "#Group by source computer, and apply the feature extractor\n",
    "out = flows.groupby('source_computer')  # This is an iterator object where each object is a dataframe grouped by the \n",
    "                                        #.....\"sourced computer\" variable\n",
    "    \n",
    "out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Using apply to literaly apply the \"featurize\" function on top on each dataframe\n",
    "    - Note the total number of rows, that means the iterator object had that many dataframes as each object!\n",
    "\"\"\"\n",
    "\n",
    "out = flows.groupby('source_computer').apply(featurize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_computer\n",
       "C10026    {'unique_ports': 2, 'average_packet': 21.0, 'a...\n",
       "C10047    {'unique_ports': 5, 'average_packet': 21.07692...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the iterator to a dataframe by calling list on it\n",
    "X = pd.DataFrame(list(out), index= out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_duration</th>\n",
       "      <th>average_packet</th>\n",
       "      <th>unique_ports</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_computer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C10026</th>\n",
       "      <td>39.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 average_duration  average_packet  unique_ports\n",
       "source_computer                                                \n",
       "C10                           5.0           222.0             4\n",
       "C10026                       39.0            21.0             2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = ['C9945', 'C9723', 'C977', 'C10']\n",
    "\n",
    "# Check which sources in X.index are bad to create labels\n",
    "y = [x in bads for x in X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966414564404514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# Report the average accuracy of Adaboost over 3-fold CV\n",
    "print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advanced Feature Engineering\n",
    "\n",
    "- Section in focus covers noteworthy ways in which to data wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966414564404514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SECTION IN FOCUS------------------------------\n",
    "# Create a feature counting unique protocols per source\n",
    "protocols = flows.groupby('source_computer').apply(\n",
    "  lambda df: len(set(df.protocol)) )\n",
    "\n",
    "\"\"\"\n",
    "1. First the groupby simply creates an iterator object where each object is a df\n",
    "2. next you use apply and lambda to calculate the number of unique values in the \"protocol\" column in EACH object/df !\n",
    "\n",
    "So now each row has source_computer or ID and the unique protocols which is then added onto the main feature set\n",
    "\"\"\"\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "# Convert this feature into a dataframe, naming the column\n",
    "protocols_DF = pd.DataFrame(\n",
    "  protocols, index=protocols.index, columns=['protocol'])\n",
    "\n",
    "# Now concatenate this feature with the previous dataset, X\n",
    "X_more = pd.concat([X, protocols_DF], axis=1)  # could also use merge...\n",
    "\n",
    "# Refit the classifier and report its accuracy\n",
    "print(np.mean(cross_val_score(\n",
    "  AdaBoostClassifier(), X_more, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting implementation of the uniqly package! Figure out how to iterate over all columns in one passing\n",
    "\n",
    "protocols1 = flows.groupby('source_computer').apply(\n",
    "  lambda df: list(set(df.protocol)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sample Weights \n",
    "\n",
    "The idea here is that we can weight or give importance to some labels more than others! \n",
    "\n",
    "- For example: : _\"One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as \"noisy\". Thankfully you know how to do that, using weighted learning. \"_\n",
    "\n",
    "    So here essentially in our entire training dataset the first 100 labels are \"iffy\" but the rest of the labels from 101 to whatever are fine! So we can still use this model by applying weights to the first 100 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n",
      "0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Lets say that the first 100 labels were labeled using a heuristic process or some unreliable process\n",
    "y_train_noisy = y_train  # assuming right\n",
    "\n",
    "# Fit a classifier to the training data\n",
    "clf = RandomForestClassifier().fit(X_train, y_train_noisy)\n",
    "\n",
    "# Report its accuracy on the test data\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Section in focus----------------------------------\n",
    "    # Assign half the weight to the first 100 noisy examples\n",
    "weights = [0.5]*100 + [1.0]*(len(y_train)-100)\n",
    "\n",
    "\"\"\"\n",
    "Here we set the weights of the first 100 to 0.5 and the rest after 100 to 1 which means normal!\n",
    "- The weights are assigned arbitarily and intuitively \n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "# Refit using weights and report accuracy. Has it improved?\n",
    "clf_weights = RandomForestClassifier().fit(X_train, y_train_noisy, sample_weight=weights)\n",
    "print(accuracy_score(y_test, clf_weights.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Classification Metrics: Confusion Matrix \n",
    "\n",
    "- Quick runthrough of how to go about returning classification metrics using SKlearn.\n",
    "\n",
    "**Context**  _a customer who defaulted on their loan, and a \"negative\" means a customer who continued to pay without problems. The bank manager informed you that the bank makes 10K profit on average from each \"good risk\" customer, but loses 150K from each \"bad risk\" customer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote the concept of cost: In our case, since a bad customer identified as good is more expensive than a good customer identified \\nas good, the cost value above assigns a monetary value ($-thousands) to the overall classficiation metrics\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a random forest classifier to the training data\n",
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "\n",
    "# Label the test data\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Section in focus------------------------------------\n",
    "    # Get confusion matrix only\n",
    "confusion_matrix(y_test, preds)\n",
    "\n",
    "    # Get false positives/negatives from the confusion matrix\n",
    "tp, fp, fn, tn = confusion_matrix(y_test, preds).ravel()  # using ravel flattens the confusion matrix \n",
    "\n",
    "# Now compute the cost using the manager's advice\n",
    "cost = fp*10 + fn*150\n",
    "\n",
    "\"\"\"\n",
    "Note the concept of cost: In our case, since a bad customer identified as good is more expensive than a good customer identified \n",
    "as good, the cost value above assigns a monetary value ($-thousands) to the overall classficiation metrics\n",
    "\"\"\"\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optimizing Classification Thresholds\n",
    "\n",
    "The concept of optimizing classification thresholds refers to the idea that in order to reduce false positives or fine tune your confusion matrix properties- you can manually specify or iterate over different threshold values and use that to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "scores = clf.predict_proba(X_test)\n",
    "y_test = y_test == 'bad'\n",
    "\n",
    "# Section in focus ------------------------------------\n",
    "# Create a range of equally spaced threshold values\n",
    "t_range = [0,0.25,0.5,0.75,1]\n",
    "\n",
    "# Store the predicted labels for each value of the threshold\n",
    "preds = [[s[1] > thr for s in scores] for thr in t_range]\n",
    "\n",
    "# Compute the accuracy for each threshold\n",
    "accuracies = [accuracy_score(y_test, p) for p in preds]\n",
    "\n",
    "# Compute the F1 score for each threshold\n",
    "f1_scores = [f1_score(y_test, p) for p in preds]\n",
    "\n",
    "\"\"\"\n",
    "1. First we create a list of threshold values to iterate over\n",
    "2. In a nested list comprehension we use the prediction probabiities we got from predict_proba and \n",
    "simply assign 0 or 1 if the predicted probability of the class being 1 > threshold\n",
    "3. Then we calculate the accuracy score for each and figure out which threshold yielded the best accu and f1scores!\n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "# Report the optimal threshold for accuracy, and for F1\n",
    "print(t_range[np.argmax(accuracies)], t_range[np.argmax(f1_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------- -------------- -------------- -------------- --------------\n",
    "\n",
    "# Chapter 3 \n",
    "- Model Life Cycle Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Machine Learning Pipelines\n",
    "\n",
    "The big idea here is to be able to iterate over different versions of models, within which you iterate over >1 types of hyperparamters and the get the best one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10,20],\n",
    "    'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialize the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "1. First we set up a pipeline instance wherein we want to iterate over 2 things: number of continous features & \n",
    "...the n_estimator hyperparameter of the RandomForestClassifier model. \n",
    "    \n",
    "2. Then we create the parameters that will be iterated OVER. \n",
    "    Example: f\n",
    "    - feature_selection__k will have 10 and 20 iterated over as the K/number of features to select using SelectKBest(f_classif). \n",
    "    - clf__n_estimators: same here the n_estimator parameter iterates over values 2,5  \n",
    "\n",
    "    Keep in mind:\n",
    "    => While naming parameters always to \"pipelineName__K\"\n",
    "    => Note that all possible combinations are covered! So dont include lots of pipeline features as it will drastically \n",
    "    push the computational expense!\n",
    "\"\"\"\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train,y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing the metric used in the pipeline\n",
    "\n",
    "By default the pipeline picks the best combination of hyper parameters that yields the highest accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train == 'bad'\n",
    "\n",
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10,20],\n",
    "    'clf__n_estimators':[2, 5]}\n",
    "\n",
    "#Section in focus ------------------------------------\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialize the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "\"\"\"\n",
    "Note: \n",
    "    1. Here we make use our custom chosen metric using the make_scorer\n",
    "    2 then we pass it into the gridsearch process which later will yield the best\n",
    "    hyper -parameters for the optimized value of the metric fed in! \n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Deploying Machine Learning in Production Environments\n",
    "\n",
    "In this section we will learn how to deploy an ML model into production by using Pickle. Note, although we've already done this using Joblib but in this section we will see interesting ways in how SKlearn's pipeline feature is useful in not only saving the model parameters but also the data transformations that preceed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nNote:\\n    1. first we save our model in a binary Pickle file. Same as you would Joblib. Note the 'wb' means write binary\\n    2. Next we load and predict. \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "#Section in focus-------------------------------------\n",
    "\n",
    "    # Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "    \n",
    "    # Now load the model from file in the production environment\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "    # Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "    1. first we save our model in a binary Pickle file. Same as you would Joblib. Note the 'wb' means write binary\n",
    "    2. Next we load and predict. \n",
    "\"\"\"\n",
    "\n",
    "#-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Data Transformations in Production\n",
    "\n",
    "This section shows how to apply the neccessary data transformations needed on production/out-sample data before feeding it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Section in focus------------------------------\n",
    "\n",
    "    # Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):    \n",
    "    Z = X.copy()\n",
    "    Z[:,1] = Z[:,1] > multiplier*np.mean(Z[:,1])\n",
    "    return Z\n",
    "\n",
    "    # Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "    # Optimize the parameter multiplier using GridSearchCV\n",
    "params = {\n",
    "   'ft__multiplier':[0.5,1,2,3]}\n",
    "\n",
    "\"\"\"\n",
    "Note: \n",
    "    1. Here we first define a custom transformation function called more_than_average that deals with the second column/value\n",
    "    of an incoming numpy matrix or array\n",
    "    2. We pass that in the pipeline using FunctionTransformer \n",
    "    3. Finally we use the params grid search method to iterate over this multiplier to check which transformation yields\n",
    "    the best metric!\n",
    "\"\"\"\n",
    "\n",
    "#--------------------------------------------------\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agile Software Development Practices in Model Deployment\n",
    "\n",
    "\n",
    "**Context:** _Having pushed your random forest to production, you suddenly worry that a naive Bayes classifier might be better. You want to run a champion-challenger test, by comparing a naive Bayes, acting as the challenger, to exactly the model which is currently in production, which you will load from file to make sure there is no confusion. You will use the F1 score for assessment_. \n",
    "\n",
    "\n",
    "In this section we will look at how to go about switching out models in production with relative ease. Our context suggests that an NB model may be better and worth taking a look. This is what we call a _\"challenger\"_ model, since we're sort of challenging the current model in production. The current model in production is a RandomForestClassifier which we can refer to as the _\"champion\"_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294117647058824\n",
      "0.46428571428571425\n"
     ]
    }
   ],
   "source": [
    "# Load the CURRENT model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data. \n",
    "challenger = GaussianNB().fit(X_train, y_train)  # The NEW OR CHALLENGER model to try out\n",
    "\n",
    "# Section in focus ----------------------------------------\n",
    "    # Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "    1. simply we load up both models - champ and challenger\n",
    "    2. compare f1 scores and save into pickle (hence deploy) the best performing one\n",
    "\"\"\"\n",
    "\n",
    "#----------------------------------------------------------\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Production level phenomenons: Dataset shift & Domai Shift\n",
    "\n",
    "Datashifts are very interesting phenonmenons that occur when model results/predictions in a production environment do not replicate the quality of the results in development environment.\n",
    "\n",
    "2 Reasons as to why: \n",
    "\n",
    "1. **Dataset shift:** This is caused due to temporal changes in the production data. For instance: if you trained on a dataset before of a company BEFORE they made some changes in the way they calculate their features then you can see this diff in the production environment\n",
    "\n",
    "    How to spot this? : Using the concept of **window slides** where basically you take different sections of the training data pertaining to different time period and test it out  **(EXPAND ON THIS)**\n",
    "\n",
    "\n",
    "2. Domain Shift: Intuively easier to understand. If you trained your model on all adult males and then in production you predicted on female adults and kids the predictions would be off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arrh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fe32610f56a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Define sliding window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msliding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_now\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mw_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mt_now\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Extract X and y from the sliding window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'arrh' is not defined"
     ]
    }
   ],
   "source": [
    "wrange = [1,2,3]\n",
    "\n",
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "# Section in focus----------------------------    \n",
    "    \n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now -w_size+1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', 1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "\"\"\"\n",
    "Note: \n",
    "    1. We loop over different sliding windows / sections of the entire dataset\n",
    "    2. Next we fit the X, Y of that WINDOW\n",
    "    3. predict and append the f1 score\n",
    "    4. Finally we get the value of the window that gave the best f1_score / metric \n",
    "\"\"\"\n",
    "    \n",
    "#------------------------------------------    \n",
    "    \n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------- -------------- -------------- -------------- --------------\n",
    "\n",
    "# Chapter 4 \n",
    "- Unsupervised Workflows refer to implementing ML workflows in the space of unsupervised algorithms. The approaches to supervised and unsupervised, although overlapping, can starkly differ in some aspects of model fitting and training. Lets explore! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Local Anomaly Detection algorithm\n",
    "\n",
    "This section covers a simple LOCAL anomaly detection algorithm wherein each point is observed in relation to the datapoints around it and if its \"alone\" then its an outlier. This is different from a global anamoly detection system wherein outliers outside the range are considered to be anamolies\n",
    "\n",
    "\n",
    "_Context_ :\n",
    "------------------\n",
    "\n",
    "a. Outlier detection:\n",
    " \tThe training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.\n",
    "novelty detection:\n",
    " \tThe training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty.    \n",
    "    \n",
    "    \n",
    "b. Novelty detection:\n",
    " \tThe training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty.    \n",
    "    \n",
    "[Sklearn Documentation](https://scikit-learn.org/stable/modules/outlier_detection.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1]\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x = [1]*30\n",
    "x.append(10)\n",
    "\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X))\n",
    "\n",
    "# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained\n",
    "x = [1.0]*30\n",
    "x.append(10.0)  # So this 10 is the anomaly\n",
    "\n",
    "# Cast to a data frame\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X))  # if 1 then its normal, if -1 then  the datapoint is an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novelty Anomaly Detection algorithm\n",
    "\n",
    "Novelty anomaly detection algorithms simply focus on predicting on newor out-sample datasets. Theres different types including one-class SVM, local novelty detection (**shown below**) , Isolation forests and others. \n",
    "\n",
    "**The concept of contamination:**  _The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the decision function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil Desai\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a list of thirty 1s and cast to a dataframe\n",
    "X = pd.DataFrame([1.0]*30)\n",
    "\n",
    "# Create an instance of a lof novelty detector\n",
    "detector = lof(novelty=True)\n",
    "\n",
    "# Fit the detector to the data\n",
    "detector.fit(X)\n",
    "\n",
    "# Use it to predict the label of an example with value 10.0\n",
    "print(detector.predict(pd.DataFrame([10])))  # new test datapoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing thresholds using Score Samples\n",
    "\n",
    "The `score_samples(X_test)` method allows you get raw scores for the values in test being outliers. The LOWER the score, the more anomalous the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a one-class SVM detector and score the test data\n",
    "nov_det = onesvm().fit(X_train)\n",
    "\n",
    "\n",
    "# Section in focus-----------------------------------\n",
    "scores = nov_det.score_samples(X_test)\n",
    "\n",
    "# Find the observed proportion of outliers in the test data\n",
    "prop = np.mean(y_test==1)\n",
    "\n",
    "# Compute the appropriate threshold\n",
    "threshold = np.quantile(scores, prop)\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "    - scores = the raw scores for anomaly. Low = more anomolos\n",
    "    - we find the proportion of anomalies in the y of the test dataset\n",
    "    - in scores if the value is above the threshold then its an anomaly. This is where the custom comes into play\n",
    "\"\"\"\n",
    "#---------------------------------------------------------\n",
    "\n",
    "\n",
    "# Print the confusion matrix for the thresholded scores\n",
    "print(confusion_matrix(y_test, scores > threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Distance & Similarity\n",
    "\n",
    "In this sectionn we will cover how to use distance metrics and incorporate them as parameters in the outlier model fitting process. Why? because the any _local outlier factor algorithm depends a lot on the idea of a nearest neighbor, which in turn depends on the choice of distance metric._\n",
    "\n",
    "But first a quick recap of diff distance metrics! : \n",
    "\n",
    "- a. Euclidean distance : sqrt of sum of squared diff \n",
    "- b. Chebyshev: element wise distance calculation and then the max distance is taken\n",
    "- c. Hamming distance: quite the opposite of the above 2, wherein the closest hamming distance is the furthest euclidean distance. Example: vectors incorporating product IDs between lets say 2 customers. Similar purchases will be grouped together\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the dataframe\n",
    "features = X_train\n",
    "\n",
    "# Find the Euclidean distance between all pairs\n",
    "dist_eucl = dm.get_metric('euclidean').pairwise(features)\n",
    "\n",
    "# Find the Hamming distance between all pairs\n",
    "dist_hamm = dm.get_metric('hamming').pairwise(features)\n",
    "\n",
    "# Find the Chebyshev distance between all pairs\n",
    "dist_cheb = dm.get_metric('chebyshev').pairwise(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Unstructured Data\n",
    "\n",
    "Unstructured datasets are unlike the tabular datasets we've used such that there exist a fixed number of features. Some examples of unstructured datasets are : \n",
    "\n",
    "- Audio data\n",
    "- Image data (before resizing all to one size)\n",
    "- Sequence DNA data. ex: ABBB, ABBC etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stringdist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-6dd3506d0ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Compute the pairwise distance matrix in square form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquareform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_rdlevenshtein\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Run a LoF algorithm on the precomputed distance matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mpdist\u001b[1;34m(X, metric, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2028\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2029\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2030\u001b[1;33m                 \u001b[0mdm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2031\u001b[0m                 \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-6dd3506d0ce1>\u001b[0m in \u001b[0;36mmy_rdlevenshtein\u001b[1;34m(u, v)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Wrap the RD-Levenshtein metric in a custom function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_rdlevenshtein\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstringdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdlevenshtein\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Reshape the array into a numpy matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stringdist' is not defined"
     ]
    }
   ],
   "source": [
    "# Wrap the RD-Levenshtein metric in a custom function\n",
    "def my_rdlevenshtein(u, v):\n",
    "    return stringdist.rdlevenshtein(u[0], v[0])\n",
    "\n",
    "# Reshape the array into a numpy matrix\n",
    "sequences = np.array(proteins['seq']).reshape(-1, 1)\n",
    "\n",
    "# Compute the pairwise distance matrix in square form\n",
    "M = squareform(pdist(sequences, my_rdlevenshtein))\n",
    "\n",
    "# Run a LoF algorithm on the precomputed distance matrix\n",
    "preds = lof(metric='precomputed').fit_predict(M)\n",
    "\n",
    "# Compute the accuracy of the outlier predictions\n",
    "print(accuracy( (proteins['label']=='VIRUS'), (preds == -1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature encoding the first letter of the string\n",
    "proteins['first'] =  proteins['seq'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature that contains the length of the string\n",
    "proteins['len'] = proteins['seq'].str.len()\n",
    "\n",
    "# Create a feature encoding the first letter of the string\n",
    "proteins['first'] =  LabelEncoder().fit_transform(proteins['seq'].str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lof_detector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-5c9bcfe5e670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extract scores from the fitted LoF object, compute its AUC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores_lof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlof_detector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lof_detector' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract scores from the fitted LoF object, compute its AUC\n",
    "scores_lof = lof_detector.score_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8      True\n",
       "9      True\n",
       "10     True\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17    False\n",
       "18    False\n",
       "19    False\n",
       "20    False\n",
       "21    False\n",
       "22    False\n",
       "23     True\n",
       "24    False\n",
       "25    False\n",
       "26    False\n",
       "27    False\n",
       "28    False\n",
       "29    False\n",
       "      ...  \n",
       "70    False\n",
       "71    False\n",
       "72    False\n",
       "73    False\n",
       "74    False\n",
       "75    False\n",
       "76    False\n",
       "77    False\n",
       "78    False\n",
       "79    False\n",
       "80    False\n",
       "81    False\n",
       "82    False\n",
       "83    False\n",
       "84    False\n",
       "85    False\n",
       "86    False\n",
       "87     True\n",
       "88     True\n",
       "89     True\n",
       "90     True\n",
       "91     True\n",
       "92    False\n",
       "93    False\n",
       "94    False\n",
       "95     True\n",
       "96     True\n",
       "97    False\n",
       "98    False\n",
       "99    False\n",
       "Name: label, Length: 100, dtype: bool"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins['label']=='VIRUS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
