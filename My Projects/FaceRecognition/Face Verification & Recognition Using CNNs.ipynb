{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Verification\n",
    "\n",
    "The basic idea behind face recognition can be illustrated using the example below where I employ the use of the following: \n",
    "- Transfer learning: We use the VGG 16 model and the VGG Face model. Freeze layers along with modifying the neural network architecture\n",
    "- Use distance functions (euclidean and cosine) to determine matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model, model_from_json, Model\n",
    "from keras.layers import Activation , Dropout\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.convolutional import *\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning \n",
    "\n",
    "Lets load in the VGG16 pretrained model. Here we do the following: \n",
    "- (i) Load in the model\n",
    "- (ii) Modify architecture: remove last 2 dense layers. Add softmax activation on the resultant last layer\n",
    "- (iii) Freeze VGG_16 face model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Loading in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KERAS.APPLICATIONS\n",
    "- Keras has a repo of pretrained models that you can simply pull from to further your training or use it however you want\n",
    "\"\"\"\n",
    "vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Lets check out the pre-trained model we're going to tune!\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Modify model architecture\n",
    "- remove last 3 dense layers. \n",
    "- Add softmax activation\n",
    "- freezing our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform here\n",
    "face_model = Sequential()\n",
    "\n",
    "#creating our own modelfrom the VGG 16\n",
    "for layer in vgg16_model.layers[:-2]:\n",
    "    face_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding softmax activation\n",
    "face_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 117,479,232\n",
      "Trainable params: 102,764,544\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#lets view our new model\n",
    "face_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Freezing all the weights!\n",
    "\n",
    "The VGG16 model has already been pretrained on a large dataset and therefore we simply \"transfer\" the use of the model for our purpose as the convolutional neural network already has the capability of succeafully identifying image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing all weights\n",
    "for layer in face_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "We will do the following here: \n",
    "- (i) Read in our images and resize\n",
    "- (ii) represent the flattened images \n",
    "- (iii) then performing face verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Reading & resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in images and resize\n",
    "def resize(my_image):\n",
    "    img = Image.open(my_image)\n",
    "    img.load()\n",
    "    img = img.resize((224, 224), Image.ANTIALIAS)    \n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data.reshape(1,224,224,3)             #model expects 4 dims\n",
    "img1 = resize('C:/Users/Darshil/Desktop/and_darsh.jpg')\n",
    "img2 = resize('C:/Users/Darshil/Desktop/darsh.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Lets represent an image as a flattened layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4339283e-16, 6.4339283e-16, 3.6733999e-14, ..., 6.4339283e-16,\n",
       "        3.7570569e-10, 1.2395185e-13]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have simply passed the 2 images through the custom VGG16 convolutional neural network we've modified \n",
    "#model.predict(img1)\n",
    "face_model.predict(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a good match\n",
      "1.0354294 \n",
      "\n",
      "Possible match!\n",
      "0.86949027 \n",
      "\n",
      "Possible match!\n",
      "0.916277 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.916277"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define thresholds \n",
    "#epsilon = 0.40 #cosine similarity\n",
    "epsilon = 1 #euclidean distance\n",
    "\n",
    "def euclidean_distance(image1,image2, pickmodel):\n",
    "    # Lastly we can perform the face verification\n",
    "    p_img1 = pickmodel.predict(image1)\n",
    "    p_img2 = pickmodel.predict(image2)\n",
    "\n",
    "    #distance function\n",
    "    diff = p_img1 - p_img2\n",
    "    a = np.sum(np.multiply(diff,diff))\n",
    "    euc_dist = np.sqrt(a) \n",
    "        \n",
    "    if euc_dist < 1 and euc_dist > 0.5:\n",
    "        print ('Possible match!')    \n",
    "    elif euc_dist < 0.5:\n",
    "        print ('more confident match!')\n",
    "    else:\n",
    "        print ('Not a good match')\n",
    "    \n",
    "    print (euc_dist, '\\n')       \n",
    "    return euc_dist\n",
    "\n",
    "euclidean_distance(resize('leo1.jpg'),resize('darsh1.jpg'),face_model)\n",
    "euclidean_distance(resize('leo1.jpg'),resize('leo2.jpg'),face_model)\n",
    "euclidean_distance(resize('leo2.jpg'),resize('leo3.jpg'),face_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible match!\n",
      "0.8522137 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8522137"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(resize('C:/Users/Darshil/Desktop/darsh1.jpg'),resize('C:/Users/Darshil/Desktop/randface1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the VGG Face model\n",
    "\n",
    "Although the architecture is kind of similar, the VGG face model was trained on an entirely different dataset and hence has different weights. \n",
    "\n",
    "We will do the following: \n",
    "- (i) create a VGG FACE MODEL \n",
    "- (ii) Load up the weights\n",
    "- (iii) Implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_14 (ZeroPaddi (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPaddi (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPaddi (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_19 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_20 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_21 (ZeroPaddi (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_22 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_23 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_24 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_25 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_26 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we load the weights\n",
    "model.load_weights('vgg_face_weights.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00042571, 0.00037262, 0.00037893, ..., 0.00038766, 0.00037345,\n",
       "        0.00040366]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have simply passed the 2 images through the custom VGG16 FACE NETWORK\n",
    "model.predict(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more confident match!\n",
      "0.00062461285 \n",
      "\n",
      "more confident match!\n",
      "0.00020716371 \n",
      "\n",
      "more confident match!\n",
      "0.0004658442 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0004658442"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we implement and find distances!\n",
    "euclidean_distance(resize('leo1.jpg'),resize('darsh1.jpg'),model)\n",
    "euclidean_distance(resize('leo1.jpg'),resize('leo2.jpg'),model)\n",
    "euclidean_distance(resize('leo2.jpg'),resize('leo3.jpg'),model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more confident match!\n",
      "0.00043190693 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00043190693"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance(resize('leo1.jpg'),resize('me3.jpg'),model)\n",
    "#PENDING, CHANGE THRESHOLD FOR VGG FACE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an intermediary layer which doesn't seem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs=model.layers[0].input\n",
    ", outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1132349 , -0.01994853, -0.00317083, ...,  0.01962105,\n",
       "        -0.01773274,  0.06005107]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distance(resize('leo1.jpg'),resize('darsh1.jpg'),vgg_face_descriptor)\n",
    "euclidean_distance(resize('leo1.jpg'),resize('leo2.jpg'),vgg_face_descriptor)\n",
    "euclidean_distance(resize('leo2.jpg'),resize('leo3.jpg'),vgg_face_descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
