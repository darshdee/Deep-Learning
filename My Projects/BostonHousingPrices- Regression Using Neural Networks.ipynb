{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices Using Neural Networks\n",
    "\n",
    "This project will cover basics of regression using a neural network to predict housing prices, training on the Boston housing prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb\n",
    "# https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# We'll just import the boston housing dataset from sklearn.dataset\n",
    "- Remember that the training data does not include the price \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# loading in training data only \n",
    "boston_data = load_boston()\n",
    "boston_df =pd.DataFrame(boston_data.data)\n",
    "boston_df['PRICE'] = boston_data.target\n",
    "boston_df.head()\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# here we convert the dataframe into numpy matrix and define the x, y data\n",
    "boston_df_without_price = boston_df.drop(['PRICE'], axis=1)\n",
    "\n",
    "training_boston_x= boston_df_without_price.values\n",
    "training_boston_y = boston_df['PRICE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  PRICE  \n",
       "0  396.90  4.98   24.0  \n",
       "1  396.90  9.14   21.6  \n",
       "2  392.83  4.03   34.7  \n",
       "3  394.63  2.94   33.4  \n",
       "4  396.90  5.33   36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Training cost= 151.17615 W= [[ 1.08181596e-01]\n",
      " [-1.17458299e-01]\n",
      " [-1.87981236e+00]\n",
      " [ 3.17177629e+00]\n",
      " [-1.46679819e+00]\n",
      " [-2.96776503e-01]\n",
      " [ 1.10750474e-01]\n",
      " [-1.92724138e-01]\n",
      " [-1.69154310e+00]\n",
      " [ 1.18187368e-01]\n",
      " [ 5.22914529e-01]\n",
      " [-6.16591773e-04]\n",
      " [-6.81648701e-02]] b= [[-0.7991367]]\n",
      "The predicted value is:  [[24.42299]]\n"
     ]
    }
   ],
   "source": [
    "# Lets set up the computation graph here\n",
    "\n",
    "# inputs for A0\n",
    "# X = tf.placeholder(tf.float32, shape = (506, 13))\n",
    "# Y= tf.placeholder(tf.float32, shape = (506,))\n",
    "\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "# weights for output layer\n",
    "wL = tf.Variable(tf.random_normal([13,1]))\n",
    "bL = tf.Variable(tf.random_normal([1,1]))\n",
    "\n",
    "# Set up the linear model\n",
    "prediction = tf.add(tf.matmul(X,wL), bL)\n",
    "\n",
    "# Now lets set up a cost function- remember that this is a regression problem so we can use the mean squared error\n",
    "# Also number of samples\n",
    "n_samples = training_boston_x.shape[0]\n",
    "\n",
    "# Mean squared error = sum(yhat - y)**2 / m,  cost is as follows:  \n",
    "cost = tf.reduce_mean(tf.square(prediction-Y))\n",
    "\n",
    "# Gradient descent\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "lfp = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    \n",
    "#     Lets begin training    \n",
    "    for epoch in range(1000):        \n",
    "        _, loss = sess.run([optimizer,cost],  feed_dict = {X: training_boston_x, Y: training_boston_y})\n",
    "        lfp.append(loss)\n",
    "\n",
    "                                \n",
    "#         if (epoch+1) % 100 == 0:        \n",
    "#             c = sess.run(cost, feed_dict={X: training_boston_x, Y:training_boston_y})\n",
    "#             print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \"W=\", sess.run(wL), \"b=\", sess.run(bL))\n",
    "            \n",
    "    print (\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: training_boston_x, Y: training_boston_y})\n",
    "    print (\"Training cost=\", training_cost, \"W=\", sess.run(wL), \"b=\", sess.run(bL))\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    PREDICTION\n",
    "    - Now we use the trained model to predict on some inputs\n",
    "    - for now we'll just pick a row from the existing training data to make it work\n",
    "    \"\"\"    \n",
    "    \n",
    "    row = 18\n",
    "    px = training_boston_x[row,:]\n",
    "    py = training_boston_y[row]\n",
    "    print (\"The predicted value is: \", sess.run(prediction, feed_dict = {X: [px], Y: py }))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph the Loss Value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2209c524128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost function ends at:  114.08734\n"
     ]
    }
   ],
   "source": [
    "plt.plot(lfp, '-', label='Cost Value vs Number of Iterations')\n",
    "# plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "%matplotlib inline\n",
    "\n",
    "print (\"cost function ends at: \", str(lfp[len(lfp)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2    3      4      5      6       7    8      9    10  \\\n",
       "0   0.00632  18.0  2.31  0.0  0.538  6.575   65.2  4.0900  1.0  296.0  15.3   \n",
       "1   0.02731   0.0  7.07  0.0  0.469  6.421   78.9  4.9671  2.0  242.0  17.8   \n",
       "2   0.02729   0.0  7.07  0.0  0.469  7.185   61.1  4.9671  2.0  242.0  17.8   \n",
       "3   0.03237   0.0  2.18  0.0  0.458  6.998   45.8  6.0622  3.0  222.0  18.7   \n",
       "4   0.06905   0.0  2.18  0.0  0.458  7.147   54.2  6.0622  3.0  222.0  18.7   \n",
       "5   0.02985   0.0  2.18  0.0  0.458  6.430   58.7  6.0622  3.0  222.0  18.7   \n",
       "6   0.08829  12.5  7.87  0.0  0.524  6.012   66.6  5.5605  5.0  311.0  15.2   \n",
       "7   0.14455  12.5  7.87  0.0  0.524  6.172   96.1  5.9505  5.0  311.0  15.2   \n",
       "8   0.21124  12.5  7.87  0.0  0.524  5.631  100.0  6.0821  5.0  311.0  15.2   \n",
       "9   0.17004  12.5  7.87  0.0  0.524  6.004   85.9  6.5921  5.0  311.0  15.2   \n",
       "10  0.22489  12.5  7.87  0.0  0.524  6.377   94.3  6.3467  5.0  311.0  15.2   \n",
       "11  0.11747  12.5  7.87  0.0  0.524  6.009   82.9  6.2267  5.0  311.0  15.2   \n",
       "12  0.09378  12.5  7.87  0.0  0.524  5.889   39.0  5.4509  5.0  311.0  15.2   \n",
       "13  0.62976   0.0  8.14  0.0  0.538  5.949   61.8  4.7075  4.0  307.0  21.0   \n",
       "14  0.63796   0.0  8.14  0.0  0.538  6.096   84.5  4.4619  4.0  307.0  21.0   \n",
       "15  0.62739   0.0  8.14  0.0  0.538  5.834   56.5  4.4986  4.0  307.0  21.0   \n",
       "16  1.05393   0.0  8.14  0.0  0.538  5.935   29.3  4.4986  4.0  307.0  21.0   \n",
       "17  0.78420   0.0  8.14  0.0  0.538  5.990   81.7  4.2579  4.0  307.0  21.0   \n",
       "18  0.80271   0.0  8.14  0.0  0.538  5.456   36.6  3.7965  4.0  307.0  21.0   \n",
       "\n",
       "        11     12  PRICE  \n",
       "0   396.90   4.98   24.0  \n",
       "1   396.90   9.14   21.6  \n",
       "2   392.83   4.03   34.7  \n",
       "3   394.63   2.94   33.4  \n",
       "4   396.90   5.33   36.2  \n",
       "5   394.12   5.21   28.7  \n",
       "6   395.60  12.43   22.9  \n",
       "7   396.90  19.15   27.1  \n",
       "8   386.63  29.93   16.5  \n",
       "9   386.71  17.10   18.9  \n",
       "10  392.52  20.45   15.0  \n",
       "11  396.90  13.27   18.9  \n",
       "12  390.50  15.71   21.7  \n",
       "13  396.90   8.26   20.4  \n",
       "14  380.02  10.26   18.2  \n",
       "15  395.62   8.47   19.9  \n",
       "16  386.85   6.58   23.1  \n",
       "17  386.75  14.67   17.5  \n",
       "18  288.99  11.69   20.2  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras\n",
    "- We use keras so as to be able to save the model and use it inmy flask app at ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 20)                280       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 505 samples, validate on 1 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 269.7821 - acc: 0.0059 - val_loss: 110.8396 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      " - 0s - loss: 94.6700 - acc: 0.0040 - val_loss: 126.2759 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      " - 0s - loss: 79.8211 - acc: 0.0198 - val_loss: 137.2144 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      " - 0s - loss: 70.7360 - acc: 0.0099 - val_loss: 125.4722 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      " - 0s - loss: 65.6463 - acc: 0.0079 - val_loss: 169.3752 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      " - 0s - loss: 62.9687 - acc: 0.0059 - val_loss: 136.7711 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      " - 0s - loss: 61.1364 - acc: 0.0119 - val_loss: 192.9401 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      " - 0s - loss: 59.5296 - acc: 0.0059 - val_loss: 182.6280 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      " - 0s - loss: 58.4871 - acc: 0.0059 - val_loss: 173.5127 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      " - 0s - loss: 56.3256 - acc: 0.0059 - val_loss: 202.8108 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      " - 0s - loss: 55.1592 - acc: 0.0079 - val_loss: 226.5536 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      " - 0s - loss: 54.3073 - acc: 0.0079 - val_loss: 195.9678 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      " - 0s - loss: 52.6723 - acc: 0.0099 - val_loss: 219.1369 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      " - 0s - loss: 49.9433 - acc: 0.0059 - val_loss: 177.4616 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      " - 0s - loss: 50.1994 - acc: 0.0079 - val_loss: 180.4555 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      " - 0s - loss: 47.0508 - acc: 0.0059 - val_loss: 268.9644 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      " - 0s - loss: 47.0446 - acc: 0.0020 - val_loss: 190.6085 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      " - 0s - loss: 46.5650 - acc: 0.0139 - val_loss: 207.0365 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      " - 0s - loss: 43.3145 - acc: 0.0079 - val_loss: 184.6622 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      " - 0s - loss: 42.0703 - acc: 0.0119 - val_loss: 265.5388 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      " - 0s - loss: 40.9977 - acc: 0.0059 - val_loss: 176.8190 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      " - 0s - loss: 39.2879 - acc: 0.0040 - val_loss: 234.0181 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      " - 0s - loss: 37.8020 - acc: 0.0119 - val_loss: 176.1612 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      " - 0s - loss: 37.0144 - acc: 0.0119 - val_loss: 324.2985 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      " - 0s - loss: 36.2156 - acc: 0.0178 - val_loss: 219.5752 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      " - 0s - loss: 35.6193 - acc: 0.0040 - val_loss: 280.1630 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      " - 0s - loss: 34.4894 - acc: 0.0119 - val_loss: 267.9368 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      " - 0s - loss: 33.0777 - acc: 0.0158 - val_loss: 233.6786 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      " - 0s - loss: 33.7143 - acc: 0.0119 - val_loss: 277.0199 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      " - 0s - loss: 31.4404 - acc: 0.0158 - val_loss: 251.4297 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      " - 0s - loss: 31.7863 - acc: 0.0079 - val_loss: 262.3326 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      " - 0s - loss: 30.1754 - acc: 0.0158 - val_loss: 258.4685 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      " - 0s - loss: 29.6243 - acc: 0.0119 - val_loss: 278.1366 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      " - 0s - loss: 28.9218 - acc: 0.0119 - val_loss: 262.5788 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      " - 0s - loss: 28.9361 - acc: 0.0059 - val_loss: 335.7193 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      " - 0s - loss: 28.8887 - acc: 0.0119 - val_loss: 241.8443 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      " - 0s - loss: 28.2350 - acc: 0.0040 - val_loss: 253.2052 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      " - 0s - loss: 27.2697 - acc: 0.0139 - val_loss: 225.2367 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      " - 0s - loss: 30.3857 - acc: 0.0099 - val_loss: 284.3284 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      " - 0s - loss: 27.1922 - acc: 0.0139 - val_loss: 253.3780 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " - 0s - loss: 26.0477 - acc: 0.0040 - val_loss: 255.0110 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      " - 0s - loss: 26.4608 - acc: 0.0119 - val_loss: 323.9483 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      " - 0s - loss: 26.3825 - acc: 0.0158 - val_loss: 308.2437 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      " - 0s - loss: 26.6510 - acc: 0.0059 - val_loss: 310.7464 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      " - 0s - loss: 26.1805 - acc: 0.0099 - val_loss: 219.0647 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      " - 0s - loss: 25.0291 - acc: 0.0099 - val_loss: 203.3064 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      " - 0s - loss: 24.4709 - acc: 0.0099 - val_loss: 263.8993 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      " - 0s - loss: 24.4018 - acc: 0.0119 - val_loss: 203.6514 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      " - 0s - loss: 26.1817 - acc: 0.0099 - val_loss: 209.6359 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      " - 0s - loss: 25.1560 - acc: 0.0099 - val_loss: 227.5068 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      " - 0s - loss: 23.3660 - acc: 0.0139 - val_loss: 220.9726 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      " - 0s - loss: 24.3794 - acc: 0.0079 - val_loss: 256.1059 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      " - 0s - loss: 24.0075 - acc: 0.0079 - val_loss: 265.5435 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      " - 0s - loss: 22.8562 - acc: 0.0119 - val_loss: 213.2807 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      " - 0s - loss: 22.2586 - acc: 0.0079 - val_loss: 198.9657 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      " - 0s - loss: 21.9713 - acc: 0.0119 - val_loss: 254.2551 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      " - 0s - loss: 23.1153 - acc: 0.0099 - val_loss: 276.5981 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      " - 0s - loss: 22.1460 - acc: 0.0119 - val_loss: 241.3961 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      " - 0s - loss: 22.9866 - acc: 0.0059 - val_loss: 202.9482 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      " - 0s - loss: 21.3841 - acc: 0.0099 - val_loss: 196.6013 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      " - 0s - loss: 21.8503 - acc: 0.0158 - val_loss: 249.7361 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      " - 0s - loss: 21.4727 - acc: 0.0059 - val_loss: 161.1692 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      " - 0s - loss: 21.2478 - acc: 0.0099 - val_loss: 234.0540 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      " - 0s - loss: 21.2530 - acc: 0.0059 - val_loss: 215.2972 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      " - 0s - loss: 20.6483 - acc: 0.0119 - val_loss: 192.9044 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      " - 0s - loss: 20.8494 - acc: 0.0178 - val_loss: 214.7254 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      " - 0s - loss: 21.9332 - acc: 0.0099 - val_loss: 237.8259 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      " - 0s - loss: 20.8381 - acc: 0.0139 - val_loss: 201.8448 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      " - 0s - loss: 20.2133 - acc: 0.0139 - val_loss: 177.7545 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      " - 0s - loss: 20.8524 - acc: 0.0079 - val_loss: 162.1865 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      " - 0s - loss: 20.0882 - acc: 0.0119 - val_loss: 173.5415 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      " - 0s - loss: 20.9491 - acc: 0.0079 - val_loss: 159.8778 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      " - 0s - loss: 20.0066 - acc: 0.0099 - val_loss: 203.1718 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      " - 0s - loss: 19.8583 - acc: 0.0119 - val_loss: 223.1630 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      " - 0s - loss: 20.5384 - acc: 0.0178 - val_loss: 169.3031 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      " - 0s - loss: 19.1518 - acc: 0.0059 - val_loss: 152.7961 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      " - 0s - loss: 19.9246 - acc: 0.0158 - val_loss: 185.3871 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      " - 0s - loss: 19.4005 - acc: 0.0158 - val_loss: 155.9852 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      " - 0s - loss: 19.2067 - acc: 0.0099 - val_loss: 162.0762 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      " - 0s - loss: 18.7811 - acc: 0.0158 - val_loss: 235.3734 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      " - 0s - loss: 22.5737 - acc: 0.0079 - val_loss: 191.3022 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      " - 0s - loss: 18.8121 - acc: 0.0099 - val_loss: 186.1555 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      " - 0s - loss: 18.6704 - acc: 0.0158 - val_loss: 172.3790 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      " - 0s - loss: 19.2443 - acc: 0.0158 - val_loss: 121.5010 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      " - 0s - loss: 19.2238 - acc: 0.0099 - val_loss: 147.9628 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      " - 0s - loss: 17.8783 - acc: 0.0158 - val_loss: 215.3450 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      " - 0s - loss: 18.1619 - acc: 0.0139 - val_loss: 189.9084 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      " - 0s - loss: 18.7671 - acc: 0.0178 - val_loss: 146.4711 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      " - 0s - loss: 19.1318 - acc: 0.0158 - val_loss: 171.6367 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      " - 0s - loss: 18.1661 - acc: 0.0079 - val_loss: 196.0828 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      " - 0s - loss: 18.7044 - acc: 0.0099 - val_loss: 151.0351 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      " - 0s - loss: 17.9758 - acc: 0.0158 - val_loss: 161.5811 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      " - 0s - loss: 17.5975 - acc: 0.0178 - val_loss: 157.4714 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      " - 0s - loss: 17.8484 - acc: 0.0158 - val_loss: 172.6120 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      " - 0s - loss: 18.1368 - acc: 0.0079 - val_loss: 167.2424 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      " - 0s - loss: 17.9320 - acc: 0.0099 - val_loss: 143.6147 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      " - 0s - loss: 17.5226 - acc: 0.0158 - val_loss: 154.2666 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      " - 0s - loss: 18.5555 - acc: 0.0218 - val_loss: 174.2054 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      " - 0s - loss: 18.3225 - acc: 0.0139 - val_loss: 237.8085 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      " - 0s - loss: 18.2724 - acc: 0.0079 - val_loss: 142.6227 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c6863de9e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple linear NN\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Creating our DNN\n",
    "model = Sequential([\n",
    "    Dense(20,input_shape =(13,), activation = 'relu', kernel_initializer='normal'),    \n",
    "    Dense(1, kernel_initializer='normal')\n",
    "    ])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.fit(training_boston_x,training_boston_y,validation_split=0.001, epochs=100, shuffle=True, verbose=2, batch_size=10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\"\"\"\n",
    "Whats going on here: we multiply a 506,12 matrix with the coefficients/parameters of dimensions 13,1 to get y (506,1)\n",
    "\"\"\"\n",
    "\n",
    "# loading in training data only \n",
    "boston_data = load_boston()\n",
    "boston_df =pd.DataFrame(boston_data.data)\n",
    "boston_df['PRICE'] = boston_data.target\n",
    "boston_df.head()\n",
    "\n",
    "# here we convert the dataframe into numpy matrix and define the x, y data\n",
    "boston_df_without_price = boston_df.drop(['PRICE'], axis=1)\n",
    "\n",
    "training_boston_x= boston_df_without_price.values\n",
    "training_boston_y = boston_df['PRICE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call model, fit and train\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(training_boston_x,training_boston_y)\n",
    "model.predict([[2000,23232,2323,1212,344,566,555,8,9,10,1100,1200,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the scikitlearn model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boston_housing_model']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we save this model using joblib\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model,'boston_housing_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.66663278])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the model\n",
    "mj = joblib.load('boston_housing_model')\n",
    "mj.predict([[0.0005,10,3,0,0.65,7,50,5,2,290,15,300,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  PRICE  \n",
       "0  396.90  4.98   24.0  \n",
       "1  396.90  9.14   21.6  \n",
       "2  392.83  4.03   34.7  \n",
       "3  394.63  2.94   33.4  \n",
       "4  396.90  5.33   36.2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing this section for flask deployement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# We'll just import the boston housing dataset from sklearn.dataset\n",
    "- Remember that the training data does not include the price \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# loading in training data only \n",
    "boston_data = load_boston()\n",
    "boston_df =pd.DataFrame(boston_data.data)\n",
    "boston_df['PRICE'] = boston_data.target\n",
    "boston_df.head()\n",
    "\n",
    "\n",
    "# here we convert the dataframe into numpy matrix and define the x, y data\n",
    "boston_df_without_price = boston_df.drop(['PRICE'], axis=1)\n",
    "\n",
    "training_boston_x= boston_df_without_price.values\n",
    "training_boston_y = boston_df['PRICE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = training_boston_x[:,0:2]\n",
    "test_y = training_boston_y\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 495 samples, validate on 11 samples\n",
      "Epoch 1/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 11/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 12/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 13/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 14/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 15/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 16/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 17/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 18/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 19/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 20/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 21/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 22/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 23/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 24/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 25/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 26/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 27/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 28/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 29/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 30/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 31/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 32/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 33/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 34/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 35/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 36/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 37/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 38/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 39/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n",
      "Epoch 40/40\n",
      " - 0s - loss: 596.3816 - acc: 0.0000e+00 - val_loss: 401.5873 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c6daf3ef0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation = 'relu', input_shape=(2,)))\n",
    "model.summary()\n",
    "model.compile(sgd(lr=0.01), loss = 'mse', metrics=['accuracy'])\n",
    "model.fit(test_x,test_y, batch_size = 10,validation_split=0.02, epochs=40, shuffle=True, verbose=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[1,2],\n",
    "                 [2,3]]).reshape(2,2)\n",
    "predictions = model.predict(test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/Darshil/gitly/Deep-Learning/My Projects/test_boston.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "new_model = load_model('C:/Users/Darshil/gitly/Deep-Learning/My Projects/Flask_Keras/saved_models/test_boston.h5')\n",
    "test = np.array([1222,4]).reshape(1,2)\n",
    "pred = new_model.predict(test)\n",
    "float(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-5a4503bc7828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "np.array(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
