{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "\n",
    "[Read Here on OpenCV Face Detection Docs](https://www.learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/?fbclid=IwAR3Ud9c9pp1eVtwlBzQ9KyIqm2xV6YZCn3IS2Y4RMNJYP_c5vcoKSMTzdUo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import ImageGrab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_smile.xml')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while True:\n",
    "    ret, img = cap.read()  #read video\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)     #convert into grayscale \n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,5)   #read from grayscale, computationally cheaper\n",
    "    \n",
    "    #FACES\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h,x:x+w]        #get face coordinates grayscale\n",
    "        roi_color = img[y:y+h,x:x+w]        #get face coordinates color\n",
    "        cv2.putText(img,'Face spotted!', (x,y),font,0.7,(200,255,255),1,cv2.LINE_AA)  #IMAGE,text,position start,font,size,color,thickness,-\n",
    "        \n",
    "        #SMILES\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray,1.3,5) #read from grayscale, computationally cheaper\n",
    "        for (x, y, w, h) in smiles:\n",
    "            cv2.rectangle(roi_color, (x, y), (x+w, y+h), (0, 0, 255), 1) #IMAGE,text,position start,font,size,color,thickness,-\n",
    "        \n",
    "        #EYES\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)        #read from grayscale, computationally cheaper\n",
    "        for (ex,ey,ew,eh) in eyes:            \n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) #IMAGE,text,position start,font,size,color,thickness,-\n",
    "            \n",
    "    #SHOW IMAGE\n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(5)== ord('q'):        \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screen Grabs\n",
    "## Faces, eyes and smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_eye.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_smile.xml')\n",
    "car_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_licence_plate_rus_16stages.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "        \n",
    "    printscreen_pil = ImageGrab.grab()\n",
    "    printscreen_numpy1 = np.array(printscreen_pil.getdata(),dtype= np.uint8).reshape((printscreen_pil.size[1],printscreen_pil.size[0],3))\n",
    "    printscreen_numpy = cv2.cvtColor(printscreen_numpy1, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(printscreen_numpy, cv2.COLOR_BGR2GRAY)    \n",
    "    faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(printscreen_numpy,(x,y),(x+w,y+h),(255,0,0),2)     #draw rect and get coordinates\n",
    "        roi_gray = gray[y:y+h,x:x+w]\n",
    "        roi_color = printscreen_numpy[y:y+h,x:x+w]\n",
    "        cv2.putText(printscreen_numpy,'Face spotted!', (x,y),font,0.7,(0,255,255),1,cv2.LINE_AA)  #IMAGE,text,position start,font,size,color,thickness,-\n",
    "        \n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray,1.3,5) \n",
    "        for (x, y, w, h) in smiles:\n",
    "            cv2.rectangle(roi_color, (x, y), (x+w, y+h), (0, 0, 255), 1)\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)        \n",
    "        for (ex,ey,ew,eh) in eyes:            \n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    \n",
    "        \n",
    "        \n",
    "    cv2.imshow('img',printscreen_numpy)\n",
    "    if cv2.waitKey(5)== ord('q'):        \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screen Grabs - Full Body "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "body_cascade = cv2.CascadeClassifier('C:/Users/Darshil/Desktop/Dreams/haarcascade_fullbody.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "while True:\n",
    "        \n",
    "    printscreen_pil = ImageGrab.grab()   #grab screen\n",
    "    printscreen_numpy = np.array(printscreen_pil.getdata(),dtype= np.uint8).reshape((printscreen_pil.size[1],printscreen_pil.size[0],3))  #convert to numpy array\n",
    "    gray = cv2.cvtColor(printscreen_numpy, cv2.COLOR_BGR2GRAY)       #make the numpy image array grayscale\n",
    "    \n",
    "    #BODY \n",
    "    bodies = body_cascade.detectMultiScale(gray,1.3,5)               # detect bodies\n",
    "    \n",
    "    for (x,y,w,h) in bodies:\n",
    "        cv2.rectangle(printscreen_numpy,(x,y),(x+w,y+h),(255,0,0),2)   #draw rect and get coordinates \n",
    "        roi_gray = gray[y:y+h,x:x+w]                                     \n",
    "        roi_color = printscreen_numpy[y:y+h,x:x+w]\n",
    "        cv2.putText(printscreen_numpy,'Thats a person!', (x,y),font,0.7,(200,255,255),1,cv2.LINE_AA)  #IMAGE,text,position start,font,size,color,thickness,-\n",
    "        \n",
    "\n",
    "    cv2.imshow('img',cv2.cvtColor(printscreen_numpy, cv2.COLOR_BGR2RGB))\n",
    "    if cv2.waitKey(5)== ord('q'):        \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Face Detector in OpenCV\n",
    "\n",
    "Deep Learning model based on the [single shot](https://arxiv.org/abs/1512.02325) detector, using the ResNet-10 NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open ./mmod_human_face_detector.dat for reading.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-abbfa505165d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdnnFaceDetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_face_detection_model_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./mmod_human_face_detector.dat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfaceRects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnnFaceDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeDlibHogSmall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfaceRect\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaceRects\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceRect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceRect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open ./mmod_human_face_detector.dat for reading."
     ]
    }
   ],
   "source": [
    "dnnFaceDetector = dlib.cnn_face_detection_model_v1(\"./mmod_human_face_detector.dat\")\n",
    "faceRects = dnnFaceDetector(frameDlibHogSmall, 0)\n",
    "for faceRect in faceRects:\n",
    "    x1 = faceRect.rect.left()\n",
    "    y1 = faceRect.rect.top()\n",
    "    x2 = faceRect.rect.right()\n",
    "    y2 = faceRect.rect.bottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
